// ============================================================================
// AUTO-GENERATED FILE - DO NOT MODIFY MANUALLY
// Generated by: scripts/generate_conformance_tests.py
// Regenerate with: python3 scripts/generate_conformance_tests.py
// ============================================================================

// Copyright 2025 International Digital Economy Academy
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

///|
/// html5lib Tokenizer Conformance Tests (continued)
/// Source: https://github.com/html5lib/html5lib-tests

///|
test "html5lib/tokenizer/test3_a_a_6136" {
  let (tokens, _) = @html.tokenize("<a a=\"?\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"?\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6137" {
  let (tokens, _) = @html.tokenize("<a a=\"@\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"@\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aa_6138" {
  let (tokens, _) = @html.tokenize("<a a=\"A\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"A\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_ab_6139" {
  let (tokens, _) = @html.tokenize("<a a=\"B\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"B\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_ay_6140" {
  let (tokens, _) = @html.tokenize("<a a=\"Y\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"Y\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_az_6141" {
  let (tokens, _) = @html.tokenize("<a a=\"Z\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"Z\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6142" {
  let (tokens, _) = @html.tokenize("<a a=\"`\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"`\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aa_6143" {
  let (tokens, _) = @html.tokenize("<a a=\"a\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_ab_6144" {
  let (tokens, _) = @html.tokenize("<a a=\"b\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"b\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_ay_6145" {
  let (tokens, _) = @html.tokenize("<a a=\"y\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"y\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_az_6146" {
  let (tokens, _) = @html.tokenize("<a a=\"z\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"z\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6147" {
  let (tokens, _) = @html.tokenize("<a a=\"{\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"{\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_audbc0udc00_6148" {
  let (tokens, _) = @html.tokenize("<a a=\"􀀀\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"􀀀\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6149" {
  let (tokens, _) = @html.tokenize("<a a=#>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"#\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6150" {
  let (tokens, _) = @html.tokenize("<a a=%>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"%\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6151" {
  let (tokens, _) = @html.tokenize("<a a=&>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"&\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6152" {
  let (tokens, _) = @html.tokenize("<a a=''>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au0000_6153" {
  let (tokens, _) = @html.tokenize("<a a='\u{00}'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"�\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au0009_6154" {
  let (tokens, _) = @html.tokenize("<a a='\t'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\\t\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au000a_6155" {
  let (tokens, _) = @html.tokenize("<a a='\n'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\\n\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au000b_6156" {
  let (tokens, _) = @html.tokenize("<a a='\u{0b}'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\\u{0b}\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au000c_6157" {
  let (tokens, _) = @html.tokenize("<a a='\u{0c}'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\\u{0c}\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a__6158" {
  let (tokens, _) = @html.tokenize("<a a=' '>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \" \"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6159" {
  let (tokens, _) = @html.tokenize("<a a='!'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"!\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6160" {
  let (tokens, _) = @html.tokenize("<a a='\"'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\\\"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6161" {
  let (tokens, _) = @html.tokenize("<a a='%'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"%\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6162" {
  let (tokens, _) = @html.tokenize("<a a='&'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"&\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6163" {
  let (tokens, _) = @html.tokenize("<a a=''>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au0000_6164" {
  let (tokens, _) = @html.tokenize("<a a=''\u{00}>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"�\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au0008_6165" {
  let (tokens, _) = @html.tokenize("<a a=''\b>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"\\b\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au0009_6166" {
  let (tokens, _) = @html.tokenize("<a a=''\t>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au000a_6167" {
  let (tokens, _) = @html.tokenize("<a a=''\n>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au000b_6168" {
  let (tokens, _) = @html.tokenize("<a a=''\u{0b}>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"\\u{0b}\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au000c_6169" {
  let (tokens, _) = @html.tokenize("<a a=''\u{0c}>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au000d_6170" {
  let (tokens, _) = @html.tokenize("<a a=''\r>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au001f_6171" {
  let (tokens, _) = @html.tokenize("<a a=''\u{1f}>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"\\u{1f}\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a__6172" {
  let (tokens, _) = @html.tokenize("<a a='' >")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6173" {
  let (tokens, _) = @html.tokenize("<a a=''!>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"!\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6174" {
  let (tokens, _) = @html.tokenize("<a a=''\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"\\\"\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6175" {
  let (tokens, _) = @html.tokenize("<a a=''&>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"&\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6176" {
  let (tokens, _) = @html.tokenize("<a a='''>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"'\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6177" {
  let (tokens, _) = @html.tokenize("<a a=''->")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"-\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6178" {
  let (tokens, _) = @html.tokenize("<a a=''.>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \".\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6179" {
  let (tokens, _) = @html.tokenize("<a a=''/>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a0_6180" {
  let (tokens, _) = @html.tokenize("<a a=''0>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"0\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a1_6181" {
  let (tokens, _) = @html.tokenize("<a a=''1>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"1\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a9_6182" {
  let (tokens, _) = @html.tokenize("<a a=''9>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"9\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6183" {
  let (tokens, _) = @html.tokenize("<a a=''<>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"<\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6184" {
  let (tokens, _) = @html.tokenize("<a a=''=>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"=\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6185" {
  let (tokens, _) = @html.tokenize("<a a=''>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6186" {
  let (tokens, _) = @html.tokenize("<a a=''?>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"?\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6187" {
  let (tokens, _) = @html.tokenize("<a a=''@>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"@\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aa_6188" {
  let (tokens, _) = @html.tokenize("<a a=''A>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_ab_6189" {
  let (tokens, _) = @html.tokenize("<a a=''B>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"b\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_ay_6190" {
  let (tokens, _) = @html.tokenize("<a a=''Y>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"y\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_az_6191" {
  let (tokens, _) = @html.tokenize("<a a=''Z>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"z\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6192" {
  let (tokens, _) = @html.tokenize("<a a=''`>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"`\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aa_6193" {
  let (tokens, _) = @html.tokenize("<a a=''a>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_ab_6194" {
  let (tokens, _) = @html.tokenize("<a a=''b>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"b\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_ay_6195" {
  let (tokens, _) = @html.tokenize("<a a=''y>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"y\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_az_6196" {
  let (tokens, _) = @html.tokenize("<a a=''z>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"z\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6197" {
  let (tokens, _) = @html.tokenize("<a a=''{>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"{\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_audbc0udc00_6198" {
  let (tokens, _) = @html.tokenize("<a a=''􀀀>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"􀀀\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6199" {
  let (tokens, _) = @html.tokenize("<a a='('>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"(\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6200" {
  let (tokens, _) = @html.tokenize("<a a='-'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"-\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6201" {
  let (tokens, _) = @html.tokenize("<a a='/'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"/\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a0_6202" {
  let (tokens, _) = @html.tokenize("<a a='0'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"0\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a1_6203" {
  let (tokens, _) = @html.tokenize("<a a='1'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"1\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a9_6204" {
  let (tokens, _) = @html.tokenize("<a a='9'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"9\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6205" {
  let (tokens, _) = @html.tokenize("<a a='<'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"<\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6206" {
  let (tokens, _) = @html.tokenize("<a a='='>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"=\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6207" {
  let (tokens, _) = @html.tokenize("<a a='>'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \">\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6208" {
  let (tokens, _) = @html.tokenize("<a a='?'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"?\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6209" {
  let (tokens, _) = @html.tokenize("<a a='@'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"@\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aa_6210" {
  let (tokens, _) = @html.tokenize("<a a='A'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"A\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_ab_6211" {
  let (tokens, _) = @html.tokenize("<a a='B'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"B\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_ay_6212" {
  let (tokens, _) = @html.tokenize("<a a='Y'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"Y\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_az_6213" {
  let (tokens, _) = @html.tokenize("<a a='Z'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"Z\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6214" {
  let (tokens, _) = @html.tokenize("<a a='`'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"`\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aa_6215" {
  let (tokens, _) = @html.tokenize("<a a='a'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_ab_6216" {
  let (tokens, _) = @html.tokenize("<a a='b'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"b\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_ay_6217" {
  let (tokens, _) = @html.tokenize("<a a='y'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"y\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_az_6218" {
  let (tokens, _) = @html.tokenize("<a a='z'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"z\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6219" {
  let (tokens, _) = @html.tokenize("<a a='{'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"{\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_audbc0udc00_6220" {
  let (tokens, _) = @html.tokenize("<a a='􀀀'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"􀀀\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6221" {
  let (tokens, _) = @html.tokenize("<a a=(>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"(\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6222" {
  let (tokens, _) = @html.tokenize("<a a=->")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"-\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6223" {
  let (tokens, _) = @html.tokenize("<a a=/>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"/\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a0_6224" {
  let (tokens, _) = @html.tokenize("<a a=0>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"0\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a1_6225" {
  let (tokens, _) = @html.tokenize("<a a=1>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"1\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a9_6226" {
  let (tokens, _) = @html.tokenize("<a a=9>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"9\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6227" {
  let (tokens, _) = @html.tokenize("<a a=<>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"<\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6228" {
  let (tokens, _) = @html.tokenize("<a a==>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"=\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6229" {
  let (tokens, _) = @html.tokenize("<a a=>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6230" {
  let (tokens, _) = @html.tokenize("<a a=?>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"?\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6231" {
  let (tokens, _) = @html.tokenize("<a a=@>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"@\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aa_6232" {
  let (tokens, _) = @html.tokenize("<a a=A>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"A\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_ab_6233" {
  let (tokens, _) = @html.tokenize("<a a=B>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"B\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_ay_6234" {
  let (tokens, _) = @html.tokenize("<a a=Y>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"Y\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_az_6235" {
  let (tokens, _) = @html.tokenize("<a a=Z>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"Z\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6236" {
  let (tokens, _) = @html.tokenize("<a a=`>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"`\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aa_6237" {
  let (tokens, _) = @html.tokenize("<a a=a>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aau0000_6238" {
  let (tokens, _) = @html.tokenize("<a a=a\u{00}>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a�\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aau0008_6239" {
  let (tokens, _) = @html.tokenize("<a a=a\b>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a\\b\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aau0009_6240" {
  let (tokens, _) = @html.tokenize("<a a=a\t>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aau000a_6241" {
  let (tokens, _) = @html.tokenize("<a a=a\n>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aau000b_6242" {
  let (tokens, _) = @html.tokenize("<a a=a\u{0b}>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a\\u{0b}\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aau000c_6243" {
  let (tokens, _) = @html.tokenize("<a a=a\u{0c}>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aau000d_6244" {
  let (tokens, _) = @html.tokenize("<a a=a\r>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aau001f_6245" {
  let (tokens, _) = @html.tokenize("<a a=a\u{1f}>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a\\u{1f}\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aa__6246" {
  let (tokens, _) = @html.tokenize("<a a=a >")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aa_6247" {
  let (tokens, _) = @html.tokenize("<a a=a!>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a!\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aa_6248" {
  let (tokens, _) = @html.tokenize("<a a=a\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a\\\"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aa_6249" {
  let (tokens, _) = @html.tokenize("<a a=a#>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a#\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aa_6250" {
  let (tokens, _) = @html.tokenize("<a a=a%>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a%\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aa_6251" {
  let (tokens, _) = @html.tokenize("<a a=a&>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a&\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aa_6252" {
  let (tokens, _) = @html.tokenize("<a a=a'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a'\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aa_6253" {
  let (tokens, _) = @html.tokenize("<a a=a(>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a(\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aa_6254" {
  let (tokens, _) = @html.tokenize("<a a=a->")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a-\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aa_6255" {
  let (tokens, _) = @html.tokenize("<a a=a/>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a/\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aa0_6256" {
  let (tokens, _) = @html.tokenize("<a a=a0>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a0\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aa1_6257" {
  let (tokens, _) = @html.tokenize("<a a=a1>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a1\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aa9_6258" {
  let (tokens, _) = @html.tokenize("<a a=a9>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a9\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aa_6259" {
  let (tokens, _) = @html.tokenize("<a a=a<>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a<\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aa_6260" {
  let (tokens, _) = @html.tokenize("<a a=a=>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a=\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aa_6261" {
  let (tokens, _) = @html.tokenize("<a a=a>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aa_6262" {
  let (tokens, _) = @html.tokenize("<a a=a?>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a?\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aa_6263" {
  let (tokens, _) = @html.tokenize("<a a=a@>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a@\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aaa_6264" {
  let (tokens, _) = @html.tokenize("<a a=aA>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"aA\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aab_6265" {
  let (tokens, _) = @html.tokenize("<a a=aB>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"aB\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aay_6266" {
  let (tokens, _) = @html.tokenize("<a a=aY>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"aY\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aaz_6267" {
  let (tokens, _) = @html.tokenize("<a a=aZ>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"aZ\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aa_6268" {
  let (tokens, _) = @html.tokenize("<a a=a`>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a`\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aaa_6269" {
  let (tokens, _) = @html.tokenize("<a a=aa>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"aa\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aab_6270" {
  let (tokens, _) = @html.tokenize("<a a=ab>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"ab\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aay_6271" {
  let (tokens, _) = @html.tokenize("<a a=ay>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"ay\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aaz_6272" {
  let (tokens, _) = @html.tokenize("<a a=az>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"az\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aa_6273" {
  let (tokens, _) = @html.tokenize("<a a=a{>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a{\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aaudbc0udc00_6274" {
  let (tokens, _) = @html.tokenize("<a a=a􀀀>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a􀀀\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_ab_6275" {
  let (tokens, _) = @html.tokenize("<a a=b>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"b\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_ay_6276" {
  let (tokens, _) = @html.tokenize("<a a=y>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"y\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_az_6277" {
  let (tokens, _) = @html.tokenize("<a a=z>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"z\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6278" {
  let (tokens, _) = @html.tokenize("<a a={>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"{\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_audbc0udc00_6279" {
  let (tokens, _) = @html.tokenize("<a a=􀀀>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"􀀀\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6280" {
  let (tokens, _) = @html.tokenize("<a a>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6281" {
  let (tokens, _) = @html.tokenize("<a a?>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a?\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6282" {
  let (tokens, _) = @html.tokenize("<a a@>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a@\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aa_6283" {
  let (tokens, _) = @html.tokenize("<a aA>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"aa\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_ab_6284" {
  let (tokens, _) = @html.tokenize("<a aB>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"ab\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_ay_6285" {
  let (tokens, _) = @html.tokenize("<a aY>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"ay\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_az_6286" {
  let (tokens, _) = @html.tokenize("<a aZ>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"az\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6287" {
  let (tokens, _) = @html.tokenize("<a a[>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a[\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6288" {
  let (tokens, _) = @html.tokenize("<a a`>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a`\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_aa_6289" {
  let (tokens, _) = @html.tokenize("<a aa>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"aa\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_ab_6290" {
  let (tokens, _) = @html.tokenize("<a ab>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"ab\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_ay_6291" {
  let (tokens, _) = @html.tokenize("<a ay>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"ay\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_az_6292" {
  let (tokens, _) = @html.tokenize("<a az>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"az\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6293" {
  let (tokens, _) = @html.tokenize("<a a{>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a{\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_audbc0udc00_6294" {
  let (tokens, _) = @html.tokenize("<a a􀀀>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a􀀀\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_b_6295" {
  let (tokens, _) = @html.tokenize("<a b>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"b\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_y_6296" {
  let (tokens, _) = @html.tokenize("<a y>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"y\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_z_6297" {
  let (tokens, _) = @html.tokenize("<a z>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"z\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a__6298" {
  let (tokens, _) = @html.tokenize("<a {>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"{\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_udbc0udc00_6299" {
  let (tokens, _) = @html.tokenize("<a 􀀀>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"􀀀\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_6300" {
  let (tokens, _) = @html.tokenize("<a!>")
  inspect(
    tokens,
    content="[StartTag(name=\"a!\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_6301" {
  let (tokens, _) = @html.tokenize("<a\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\\\"\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_6302" {
  let (tokens, _) = @html.tokenize("<a&>")
  inspect(
    tokens,
    content="[StartTag(name=\"a&\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_6303" {
  let (tokens, _) = @html.tokenize("<a'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a'\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_6304" {
  let (tokens, _) = @html.tokenize("<a->")
  inspect(
    tokens,
    content="[StartTag(name=\"a-\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_6305" {
  let (tokens, _) = @html.tokenize("<a.>")
  inspect(
    tokens,
    content="[StartTag(name=\"a.\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_6306" {
  let (tokens, _) = @html.tokenize("<a/>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[], self_closing=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_au0000_6307" {
  let (tokens, _) = @html.tokenize("<a/\u{00}>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"�\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_au0009_6308" {
  let (tokens, _) = @html.tokenize("<a/\t>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_au000a_6309" {
  let (tokens, _) = @html.tokenize("<a/\n>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_au000b_6310" {
  let (tokens, _) = @html.tokenize("<a/\u{0b}>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"\\u{0b}\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_au000c_6311" {
  let (tokens, _) = @html.tokenize("<a/\u{0c}>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a__6312" {
  let (tokens, _) = @html.tokenize("<a/ >")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_6313" {
  let (tokens, _) = @html.tokenize("<a/!>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"!\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_6314" {
  let (tokens, _) = @html.tokenize("<a/\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"\\\"\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_6315" {
  let (tokens, _) = @html.tokenize("<a/&>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"&\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_6316" {
  let (tokens, _) = @html.tokenize("<a/'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"'\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_6317" {
  let (tokens, _) = @html.tokenize("<a/->")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"-\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_6318" {
  let (tokens, _) = @html.tokenize("<a//>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[], self_closing=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a0_6319" {
  let (tokens, _) = @html.tokenize("<a/0>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"0\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a1_6320" {
  let (tokens, _) = @html.tokenize("<a/1>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"1\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a9_6321" {
  let (tokens, _) = @html.tokenize("<a/9>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"9\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_6322" {
  let (tokens, _) = @html.tokenize("<a/<>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"<\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_6323" {
  let (tokens, _) = @html.tokenize("<a/=>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"=\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_6324" {
  let (tokens, _) = @html.tokenize("<a/>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[], self_closing=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_6325" {
  let (tokens, _) = @html.tokenize("<a/?>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"?\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_6326" {
  let (tokens, _) = @html.tokenize("<a/@>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"@\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_aa_6327" {
  let (tokens, _) = @html.tokenize("<a/A>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_ab_6328" {
  let (tokens, _) = @html.tokenize("<a/B>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"b\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_ay_6329" {
  let (tokens, _) = @html.tokenize("<a/Y>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"y\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_az_6330" {
  let (tokens, _) = @html.tokenize("<a/Z>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"z\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_6331" {
  let (tokens, _) = @html.tokenize("<a/`>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"`\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_aa_6332" {
  let (tokens, _) = @html.tokenize("<a/a>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_ab_6333" {
  let (tokens, _) = @html.tokenize("<a/b>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"b\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_ay_6334" {
  let (tokens, _) = @html.tokenize("<a/y>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"y\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_az_6335" {
  let (tokens, _) = @html.tokenize("<a/z>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"z\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_6336" {
  let (tokens, _) = @html.tokenize("<a/{>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"{\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_audbc0udc00_6337" {
  let (tokens, _) = @html.tokenize("<a/􀀀>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"􀀀\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a0_6338" {
  let (tokens, _) = @html.tokenize("<a0>")
  inspect(
    tokens,
    content="[StartTag(name=\"a0\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a1_6339" {
  let (tokens, _) = @html.tokenize("<a1>")
  inspect(
    tokens,
    content="[StartTag(name=\"a1\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a9_6340" {
  let (tokens, _) = @html.tokenize("<a9>")
  inspect(
    tokens,
    content="[StartTag(name=\"a9\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_6341" {
  let (tokens, _) = @html.tokenize("<a<>")
  inspect(
    tokens,
    content="[StartTag(name=\"a<\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_6342" {
  let (tokens, _) = @html.tokenize("<a=>")
  inspect(
    tokens,
    content="[StartTag(name=\"a=\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_6343" {
  let (tokens, _) = @html.tokenize("<a>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_6344" {
  let (tokens, _) = @html.tokenize("<a?>")
  inspect(
    tokens,
    content="[StartTag(name=\"a?\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_6345" {
  let (tokens, _) = @html.tokenize("<a@>")
  inspect(
    tokens,
    content="[StartTag(name=\"a@\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_aa_6346" {
  let (tokens, _) = @html.tokenize("<aA>")
  inspect(
    tokens,
    content="[StartTag(name=\"aa\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_ab_6347" {
  let (tokens, _) = @html.tokenize("<aB>")
  inspect(
    tokens,
    content="[StartTag(name=\"ab\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_ay_6348" {
  let (tokens, _) = @html.tokenize("<aY>")
  inspect(
    tokens,
    content="[StartTag(name=\"ay\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_az_6349" {
  let (tokens, _) = @html.tokenize("<aZ>")
  inspect(
    tokens,
    content="[StartTag(name=\"az\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_6350" {
  let (tokens, _) = @html.tokenize("<a[>")
  inspect(
    tokens,
    content="[StartTag(name=\"a[\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_6351" {
  let (tokens, _) = @html.tokenize("<a`>")
  inspect(
    tokens,
    content="[StartTag(name=\"a`\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_aa_6352" {
  let (tokens, _) = @html.tokenize("<aa>")
  inspect(
    tokens,
    content="[StartTag(name=\"aa\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_ab_6353" {
  let (tokens, _) = @html.tokenize("<ab>")
  inspect(
    tokens,
    content="[StartTag(name=\"ab\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_ay_6354" {
  let (tokens, _) = @html.tokenize("<ay>")
  inspect(
    tokens,
    content="[StartTag(name=\"ay\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_az_6355" {
  let (tokens, _) = @html.tokenize("<az>")
  inspect(
    tokens,
    content="[StartTag(name=\"az\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_6356" {
  let (tokens, _) = @html.tokenize("<a{>")
  inspect(
    tokens,
    content="[StartTag(name=\"a{\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_audbc0udc00_6357" {
  let (tokens, _) = @html.tokenize("<a􀀀>")
  inspect(
    tokens,
    content="[StartTag(name=\"a􀀀\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_b_6358" {
  let (tokens, _) = @html.tokenize("<b>")
  inspect(
    tokens,
    content="[StartTag(name=\"b\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_y_6359" {
  let (tokens, _) = @html.tokenize("<y>")
  inspect(
    tokens,
    content="[StartTag(name=\"y\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_z_6360" {
  let (tokens, _) = @html.tokenize("<z>")
  inspect(
    tokens,
    content="[StartTag(name=\"z\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3__6361" {
  let (tokens, _) = @html.tokenize("<{")
  inspect(tokens, content="[Character('<'), Character('{'), EOF]")
}

///|
test "html5lib/tokenizer/test3_udbc0udc00_6362" {
  let (tokens, _) = @html.tokenize("<􀀀")
  inspect(tokens, content="[Character('<'), Character('\\u{100000}'), EOF]")
}

///|
test "html5lib/tokenizer/test4__in_attribute_name_6393" {
  let (tokens, _) = @html.tokenize("<z/0  <>")
  inspect(
    tokens,
    content="[StartTag(name=\"z\", attrs=[{name: \"0\", value: \"\"}, {name: \"<\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4__in_unquoted_attribute_value_6394" {
  let (tokens, _) = @html.tokenize("<z x=<>")
  inspect(
    tokens,
    content="[StartTag(name=\"z\", attrs=[{name: \"x\", value: \"<\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4__in_unquoted_attribute_value_6395" {
  let (tokens, _) = @html.tokenize("<z z=z=z>")
  inspect(
    tokens,
    content="[StartTag(name=\"z\", attrs=[{name: \"z\", value: \"z=z\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4__attribute_6396" {
  let (tokens, _) = @html.tokenize("<z =>")
  inspect(
    tokens,
    content="[StartTag(name=\"z\", attrs=[{name: \"=\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4__attribute_6397" {
  let (tokens, _) = @html.tokenize("<z ==>")
  inspect(
    tokens,
    content="[StartTag(name=\"z\", attrs=[{name: \"=\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4__attribute_6398" {
  let (tokens, _) = @html.tokenize("<z ===>")
  inspect(
    tokens,
    content="[StartTag(name=\"z\", attrs=[{name: \"=\", value: \"=\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4__attribute_6399" {
  let (tokens, _) = @html.tokenize("<z ====>")
  inspect(
    tokens,
    content="[StartTag(name=\"z\", attrs=[{name: \"=\", value: \"==\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4__after_ampersand_in_doublequoted_attribute_value_6400" {
  let (tokens, _) = @html.tokenize("<z z=\"&\">")
  inspect(
    tokens,
    content="[StartTag(name=\"z\", attrs=[{name: \"z\", value: \"&\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4__after_ampersand_in_doublequoted_attribute_value_6401" {
  let (tokens, _) = @html.tokenize("<z z=\"&'\">")
  inspect(
    tokens,
    content="[StartTag(name=\"z\", attrs=[{name: \"z\", value: \"&'\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4__after_ampersand_in_singlequoted_attribute_value_6402" {
  let (tokens, _) = @html.tokenize("<z z='&'>")
  inspect(
    tokens,
    content="[StartTag(name=\"z\", attrs=[{name: \"z\", value: \"&\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4__after_ampersand_in_singlequoted_attribute_value_6403" {
  let (tokens, _) = @html.tokenize("<z z='&\"'>")
  inspect(
    tokens,
    content="[StartTag(name=\"z\", attrs=[{name: \"z\", value: \"&\\\"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_text_after_bogus_character_reference_6404" {
  let (tokens, _) = @html.tokenize("<z z='&xlink_xmlns;'>bar<z>")
  inspect(
    tokens,
    content="[StartTag(name=\"z\", attrs=[{name: \"z\", value: \"&xlink_xmlns;\"}], self_closing=false), Character('b'), Character('a'), Character('r'), StartTag(name=\"z\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_text_after_hex_character_reference_6405" {
  let (tokens, _) = @html.tokenize("<z z='&#x0020; foo'>bar<z>")
  inspect(
    tokens,
    content="[StartTag(name=\"z\", attrs=[{name: \"z\", value: \"  foo\"}], self_closing=false), Character('b'), Character('a'), Character('r'), StartTag(name=\"z\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_attribute_name_starting_with__6406" {
  let (tokens, _) = @html.tokenize("<foo \"='bar'>")
  inspect(
    tokens,
    content="[StartTag(name=\"foo\", attrs=[{name: \"\\\"\", value: \"bar\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_attribute_name_starting_with__6407" {
  let (tokens, _) = @html.tokenize("<foo '='bar'>")
  inspect(
    tokens,
    content="[StartTag(name=\"foo\", attrs=[{name: \"'\", value: \"bar\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_attribute_name_containing__6408" {
  let (tokens, _) = @html.tokenize("<foo a\"b='bar'>")
  inspect(
    tokens,
    content="[StartTag(name=\"foo\", attrs=[{name: \"a\\\"b\", value: \"bar\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_attribute_name_containing__6409" {
  let (tokens, _) = @html.tokenize("<foo a'b='bar'>")
  inspect(
    tokens,
    content="[StartTag(name=\"foo\", attrs=[{name: \"a'b\", value: \"bar\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_unquoted_attribute_value_containing__6410" {
  let (tokens, _) = @html.tokenize("<foo a=b'c>")
  inspect(
    tokens,
    content="[StartTag(name=\"foo\", attrs=[{name: \"a\", value: \"b'c\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_unquoted_attribute_value_containing__6411" {
  let (tokens, _) = @html.tokenize("<foo a=b\"c>")
  inspect(
    tokens,
    content="[StartTag(name=\"foo\", attrs=[{name: \"a\", value: \"b\\\"c\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_doublequoted_attribute_value_not_followed_by_white_6412" {
  let (tokens, _) = @html.tokenize("<foo a=\"b\"c>")
  inspect(
    tokens,
    content="[StartTag(name=\"foo\", attrs=[{name: \"a\", value: \"b\"}, {name: \"c\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_singlequoted_attribute_value_not_followed_by_white_6413" {
  let (tokens, _) = @html.tokenize("<foo a='b'c>")
  inspect(
    tokens,
    content="[StartTag(name=\"foo\", attrs=[{name: \"a\", value: \"b\"}, {name: \"c\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_quoted_attribute_followed_by_permitted__6414" {
  let (tokens, _) = @html.tokenize("<br a='b'/>")
  inspect(
    tokens,
    content="[StartTag(name=\"br\", attrs=[{name: \"a\", value: \"b\"}], self_closing=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_quoted_attribute_followed_by_nonpermitted__6415" {
  let (tokens, _) = @html.tokenize("<bar a='b'/>")
  inspect(
    tokens,
    content="[StartTag(name=\"bar\", attrs=[{name: \"a\", value: \"b\"}], self_closing=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_cr_eof_after_doctype_name_6416" {
  let (tokens, _) = @html.tokenize("<!doctype html \r")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"html\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_cr_eof_in_tag_name_6417" {
  let (tokens, _) = @html.tokenize("<z\r")
  inspect(tokens, content="[EOF]")
}

///|
test "html5lib/tokenizer/test4_slash_eof_in_tag_name_6418" {
  let (tokens, _) = @html.tokenize("<z/")
  inspect(tokens, content="[EOF]")
}

///|
test "html5lib/tokenizer/test4_zero_hex_numeric_entity_6419" {
  let (tokens, _) = @html.tokenize("&#x0")
  inspect(tokens, content="[Character('�'), EOF]")
}

///|
test "html5lib/tokenizer/test4_zero_decimal_numeric_entity_6420" {
  let (tokens, _) = @html.tokenize("&#0")
  inspect(tokens, content="[Character('�'), EOF]")
}

///|
test "html5lib/tokenizer/test4_zeroprefixed_hex_numeric_entity_6421" {
  let (tokens, _) = @html.tokenize(
    "&#x000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000041;",
  )
  inspect(tokens, content="[Character('A'), EOF]")
}

///|
test "html5lib/tokenizer/test4_zeroprefixed_decimal_numeric_entity_6422" {
  let (tokens, _) = @html.tokenize(
    "&#000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000065;",
  )
  inspect(tokens, content="[Character('A'), EOF]")
}

///|
test "html5lib/tokenizer/test4_empty_hex_numeric_entities_6423" {
  let (tokens, _) = @html.tokenize("&#x &#X ")
  inspect(
    tokens,
    content="[Character('&'), Character('#'), Character('x'), Character(' '), Character('&'), Character('#'), Character('X'), Character(' '), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_invalid_digit_in_hex_numeric_entity_6424" {
  let (tokens, _) = @html.tokenize("&#xZ")
  inspect(
    tokens,
    content="[Character('&'), Character('#'), Character('x'), Character('Z'), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_empty_decimal_numeric_entities_6425" {
  let (tokens, _) = @html.tokenize("&# &#; ")
  inspect(
    tokens,
    content="[Character('&'), Character('#'), Character(' '), Character('&'), Character('#'), Character(';'), Character(' '), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_invalid_digit_in_decimal_numeric_entity_6426" {
  let (tokens, _) = @html.tokenize("&#A")
  inspect(
    tokens,
    content="[Character('&'), Character('#'), Character('A'), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_nonbmp_numeric_entity_6427" {
  let (tokens, _) = @html.tokenize("&#x10000;")
  inspect(tokens, content="[Character('𐀀'), EOF]")
}

///|
test "html5lib/tokenizer/test4_maximum_nonbmp_numeric_entity_6428" {
  let (tokens, _) = @html.tokenize("&#X10FFFF;")
  inspect(tokens, content="[Character('\\u{10ffff}'), EOF]")
}

///|
test "html5lib/tokenizer/test4_above_maximum_numeric_entity_6429" {
  let (tokens, _) = @html.tokenize("&#x110000;")
  inspect(tokens, content="[Character('�'), EOF]")
}

///|
test "html5lib/tokenizer/test4_32bit_hex_numeric_entity_6430" {
  let (tokens, _) = @html.tokenize("&#x80000041;")
  inspect(tokens, content="[Character('�'), EOF]")
}

///|
test "html5lib/tokenizer/test4_33bit_hex_numeric_entity_6431" {
  let (tokens, _) = @html.tokenize("&#x100000041;")
  inspect(tokens, content="[Character('�'), EOF]")
}

///|
test "html5lib/tokenizer/test4_33bit_decimal_numeric_entity_6432" {
  let (tokens, _) = @html.tokenize("&#4294967361;")
  inspect(tokens, content="[Character('�'), EOF]")
}

///|
test "html5lib/tokenizer/test4_65bit_hex_numeric_entity_6433" {
  let (tokens, _) = @html.tokenize("&#x10000000000000041;")
  inspect(tokens, content="[Character('�'), EOF]")
}

///|
test "html5lib/tokenizer/test4_65bit_decimal_numeric_entity_6434" {
  let (tokens, _) = @html.tokenize("&#18446744073709551681;")
  inspect(tokens, content="[Character('�'), EOF]")
}

///|
test "html5lib/tokenizer/test4_surrogate_code_point_edge_cases_6435" {
  let (tokens, _) = @html.tokenize(
    "&#xD7FF;&#xD800;&#xD801;&#xDFFE;&#xDFFF;&#xE000;",
  )
  inspect(
    tokens,
    content="[Character('퟿'), Character('�'), Character('�'), Character('�'), Character('�'), Character('\\u{e000}'), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_uppercase_start_tag_name_6436" {
  let (tokens, _) = @html.tokenize("<X>")
  inspect(
    tokens,
    content="[StartTag(name=\"x\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_uppercase_end_tag_name_6437" {
  let (tokens, _) = @html.tokenize("</X>")
  inspect(tokens, content="[EndTag(name=\"x\"), EOF]")
}

///|
test "html5lib/tokenizer/test4_uppercase_attribute_name_6438" {
  let (tokens, _) = @html.tokenize("<x X>")
  inspect(
    tokens,
    content="[StartTag(name=\"x\", attrs=[{name: \"x\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_tagattribute_name_case_edge_values_6439" {
  let (tokens, _) = @html.tokenize("<x@AZ[`az{ @AZ[`az{>")
  inspect(
    tokens,
    content="[StartTag(name=\"x@az[`az{\", attrs=[{name: \"@az[`az{\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_duplicate_differentcase_attributes_6440" {
  let (tokens, _) = @html.tokenize("<x x=1 x=2 X=3>")
  inspect(
    tokens,
    content="[StartTag(name=\"x\", attrs=[{name: \"x\", value: \"1\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_uppercase_close_tag_attributes_6441" {
  let (tokens, _) = @html.tokenize("</x X>")
  inspect(tokens, content="[EndTag(name=\"x\"), EOF]")
}

///|
test "html5lib/tokenizer/test4_duplicate_close_tag_attributes_6442" {
  let (tokens, _) = @html.tokenize("</x x x>")
  inspect(tokens, content="[EndTag(name=\"x\"), EOF]")
}

///|
test "html5lib/tokenizer/test4_permitted_slash_6443" {
  let (tokens, _) = @html.tokenize("<br/>")
  inspect(
    tokens,
    content="[StartTag(name=\"br\", attrs=[], self_closing=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_nonpermitted_slash_6444" {
  let (tokens, _) = @html.tokenize("<xr/>")
  inspect(
    tokens,
    content="[StartTag(name=\"xr\", attrs=[], self_closing=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_permitted_slash_but_in_close_tag_6445" {
  let (tokens, _) = @html.tokenize("</br/>")
  inspect(tokens, content="[EndTag(name=\"br\"), EOF]")
}

///|
test "html5lib/tokenizer/test4_doctype_public_casesensitivity_1_6446" {
  let (tokens, _) = @html.tokenize("<!DoCtYpE HtMl PuBlIc \"AbC\" \"XyZ\">")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"html\"), public_id=Some(\"AbC\"), system_id=Some(\"XyZ\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_doctype_public_casesensitivity_2_6447" {
  let (tokens, _) = @html.tokenize("<!dOcTyPe hTmL pUbLiC \"aBc\" \"xYz\">")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"html\"), public_id=Some(\"aBc\"), system_id=Some(\"xYz\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_doctype_system_casesensitivity_1_6448" {
  let (tokens, _) = @html.tokenize("<!DoCtYpE HtMl SyStEm \"XyZ\">")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"html\"), public_id=None, system_id=Some(\"XyZ\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_doctype_system_casesensitivity_2_6449" {
  let (tokens, _) = @html.tokenize("<!dOcTyPe hTmL sYsTeM \"xYz\">")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"html\"), public_id=None, system_id=Some(\"xYz\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_u0000_in_lookahead_region_after_nonmatching_charac_6450" {
  let (tokens, _) = @html.tokenize("<!doc>\u{00}")
  inspect(tokens, content="[Comment(\"doc\"), Character('\\u{00}'), EOF]")
}

///|
test "html5lib/tokenizer/test4_u0000_in_lookahead_region_6451" {
  let (tokens, _) = @html.tokenize("<!doc\u{00}")
  inspect(tokens, content="[Comment(\"doc�\"), EOF]")
}

///|
test "html5lib/tokenizer/test4_u0080_in_lookahead_region_6452" {
  let (tokens, _) = @html.tokenize("<!doc")
  inspect(tokens, content="[Comment(\"doc\"), EOF]")
}

///|
test "html5lib/tokenizer/test4_ufdd1_in_lookahead_region_6453" {
  let (tokens, _) = @html.tokenize("<!doc﷑")
  inspect(tokens, content="[Comment(\"doc﷑\"), EOF]")
}

///|
test "html5lib/tokenizer/test4_u1ffff_in_lookahead_region_6454" {
  let (tokens, _) = @html.tokenize("<!doc🿿")
  inspect(tokens, content="[Comment(\"doc🿿\"), EOF]")
}

///|
test "html5lib/tokenizer/test4_cr_followed_by_nonlf_6455" {
  let (tokens, _) = @html.tokenize("\r?")
  inspect(tokens, content="[Character('\\n'), Character('?'), EOF]")
}

///|
test "html5lib/tokenizer/test4_cr_at_eof_6456" {
  let (tokens, _) = @html.tokenize("\r")
  inspect(tokens, content="[Character('\\n'), EOF]")
}

///|
test "html5lib/tokenizer/test4_lf_at_eof_6457" {
  let (tokens, _) = @html.tokenize("\n")
  inspect(tokens, content="[Character('\\n'), EOF]")
}

///|
test "html5lib/tokenizer/test4_cr_lf_6458" {
  let (tokens, _) = @html.tokenize("\r\n")
  inspect(tokens, content="[Character('\\n'), EOF]")
}

///|
test "html5lib/tokenizer/test4_cr_cr_6459" {
  let (tokens, _) = @html.tokenize("\r\r")
  inspect(tokens, content="[Character('\\n'), Character('\\n'), EOF]")
}

///|
test "html5lib/tokenizer/test4_lf_lf_6460" {
  let (tokens, _) = @html.tokenize("\n\n")
  inspect(tokens, content="[Character('\\n'), Character('\\n'), EOF]")
}

///|
test "html5lib/tokenizer/test4_lf_cr_6461" {
  let (tokens, _) = @html.tokenize("\n\r")
  inspect(tokens, content="[Character('\\n'), Character('\\n'), EOF]")
}

///|
test "html5lib/tokenizer/test4_text_cr_cr_cr_text_6462" {
  let (tokens, _) = @html.tokenize("text\r\r\rtext")
  inspect(
    tokens,
    content="[Character('t'), Character('e'), Character('x'), Character('t'), Character('\\n'), Character('\\n'), Character('\\n'), Character('t'), Character('e'), Character('x'), Character('t'), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_doctype_publik_6463" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE html PUBLIK \"AbC\" \"XyZ\">")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"html\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_doctype_publi_6464" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE html PUBLI")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"html\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_doctype_sistem_6465" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE html SISTEM \"AbC\">")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"html\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_doctype_sys_6466" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE html SYS")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"html\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_doctype_html_xtext_6467" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE html x>text")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"html\"), public_id=None, system_id=None, force_quirks=true), Character('t'), Character('e'), Character('x'), Character('t'), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_grave_accent_in_unquoted_attribute_6468" {
  let (tokens, _) = @html.tokenize("<a a=aa`>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"aa`\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test4_eof_in_tag_name_state__6469" {
  let (tokens, _) = @html.tokenize("<a")
  inspect(tokens, content="[EOF]")
}

///|
test "html5lib/tokenizer/test4_eof_in_before_attribute_name_state_6470" {
  let (tokens, _) = @html.tokenize("<a ")
  inspect(tokens, content="[EOF]")
}

///|
test "html5lib/tokenizer/test4_eof_in_attribute_name_state_6471" {
  let (tokens, _) = @html.tokenize("<a a")
  inspect(tokens, content="[EOF]")
}

///|
test "html5lib/tokenizer/test4_eof_in_after_attribute_name_state_6472" {
  let (tokens, _) = @html.tokenize("<a a ")
  inspect(tokens, content="[EOF]")
}

///|
test "html5lib/tokenizer/test4_eof_in_before_attribute_value_state_6473" {
  let (tokens, _) = @html.tokenize("<a a =")
  inspect(tokens, content="[EOF]")
}

///|
test "html5lib/tokenizer/test4_eof_in_attribute_value_double_quoted_state_6474" {
  let (tokens, _) = @html.tokenize("<a a =\"a")
  inspect(tokens, content="[EOF]")
}

///|
test "html5lib/tokenizer/test4_eof_in_attribute_value_single_quoted_state_6475" {
  let (tokens, _) = @html.tokenize("<a a ='a")
  inspect(tokens, content="[EOF]")
}

///|
test "html5lib/tokenizer/test4_eof_in_attribute_value_unquoted_state_6476" {
  let (tokens, _) = @html.tokenize("<a a =a")
  inspect(tokens, content="[EOF]")
}

///|
test "html5lib/tokenizer/test4_eof_in_after_attribute_value_state_6477" {
  let (tokens, _) = @html.tokenize("<a a ='a'")
  inspect(tokens, content="[EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u0001_6478" {
  let (tokens, _) = @html.tokenize("\u{01}")
  inspect(tokens, content="[Character('\\u{01}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u0002_6479" {
  let (tokens, _) = @html.tokenize("\u{02}")
  inspect(tokens, content="[Character('\\u{02}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u0003_6480" {
  let (tokens, _) = @html.tokenize("\u{03}")
  inspect(tokens, content="[Character('\\u{03}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u0004_6481" {
  let (tokens, _) = @html.tokenize("\u{04}")
  inspect(tokens, content="[Character('\\u{04}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u0005_6482" {
  let (tokens, _) = @html.tokenize("\u{05}")
  inspect(tokens, content="[Character('\\u{05}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u0006_6483" {
  let (tokens, _) = @html.tokenize("\u{06}")
  inspect(tokens, content="[Character('\\u{06}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u0007_6484" {
  let (tokens, _) = @html.tokenize("\u{07}")
  inspect(tokens, content="[Character('\\u{07}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u0008_6485" {
  let (tokens, _) = @html.tokenize("\b")
  inspect(tokens, content="[Character('\\b'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u000b_6486" {
  let (tokens, _) = @html.tokenize("\u{0b}")
  inspect(tokens, content="[Character('\\u{0b}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u000e_6487" {
  let (tokens, _) = @html.tokenize("\u{0e}")
  inspect(tokens, content="[Character('\\u{0e}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u000f_6488" {
  let (tokens, _) = @html.tokenize("\u{0f}")
  inspect(tokens, content="[Character('\\u{0f}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u0010_6489" {
  let (tokens, _) = @html.tokenize("\u{10}")
  inspect(tokens, content="[Character('\\u{10}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u0011_6490" {
  let (tokens, _) = @html.tokenize("\u{11}")
  inspect(tokens, content="[Character('\\u{11}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u0012_6491" {
  let (tokens, _) = @html.tokenize("\u{12}")
  inspect(tokens, content="[Character('\\u{12}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u0013_6492" {
  let (tokens, _) = @html.tokenize("\u{13}")
  inspect(tokens, content="[Character('\\u{13}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u0014_6493" {
  let (tokens, _) = @html.tokenize("\u{14}")
  inspect(tokens, content="[Character('\\u{14}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u0015_6494" {
  let (tokens, _) = @html.tokenize("\u{15}")
  inspect(tokens, content="[Character('\\u{15}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u0016_6495" {
  let (tokens, _) = @html.tokenize("\u{16}")
  inspect(tokens, content="[Character('\\u{16}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u0017_6496" {
  let (tokens, _) = @html.tokenize("\u{17}")
  inspect(tokens, content="[Character('\\u{17}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u0018_6497" {
  let (tokens, _) = @html.tokenize("\u{18}")
  inspect(tokens, content="[Character('\\u{18}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u0019_6498" {
  let (tokens, _) = @html.tokenize("\u{19}")
  inspect(tokens, content="[Character('\\u{19}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u001a_6499" {
  let (tokens, _) = @html.tokenize("\u{1a}")
  inspect(tokens, content="[Character('\\u{1a}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u001b_6500" {
  let (tokens, _) = @html.tokenize("\u{1b}")
  inspect(tokens, content="[Character('\\u{1b}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u001c_6501" {
  let (tokens, _) = @html.tokenize("\u{1c}")
  inspect(tokens, content="[Character('\\u{1c}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u001d_6502" {
  let (tokens, _) = @html.tokenize("\u{1d}")
  inspect(tokens, content="[Character('\\u{1d}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u001e_6503" {
  let (tokens, _) = @html.tokenize("\u{1e}")
  inspect(tokens, content="[Character('\\u{1e}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u001f_6504" {
  let (tokens, _) = @html.tokenize("\u{1f}")
  inspect(tokens, content="[Character('\\u{1f}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u007f_6505" {
  let (tokens, _) = @html.tokenize("\u{7f}")
  inspect(tokens, content="[Character('\\u{7f}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufdd0_6506" {
  let (tokens, _) = @html.tokenize("﷐")
  inspect(tokens, content="[Character('\\u{fdd0}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufdd1_6507" {
  let (tokens, _) = @html.tokenize("﷑")
  inspect(tokens, content="[Character('\\u{fdd1}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufdd2_6508" {
  let (tokens, _) = @html.tokenize("﷒")
  inspect(tokens, content="[Character('\\u{fdd2}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufdd3_6509" {
  let (tokens, _) = @html.tokenize("﷓")
  inspect(tokens, content="[Character('\\u{fdd3}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufdd4_6510" {
  let (tokens, _) = @html.tokenize("﷔")
  inspect(tokens, content="[Character('\\u{fdd4}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufdd5_6511" {
  let (tokens, _) = @html.tokenize("﷕")
  inspect(tokens, content="[Character('\\u{fdd5}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufdd6_6512" {
  let (tokens, _) = @html.tokenize("﷖")
  inspect(tokens, content="[Character('\\u{fdd6}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufdd7_6513" {
  let (tokens, _) = @html.tokenize("﷗")
  inspect(tokens, content="[Character('\\u{fdd7}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufdd8_6514" {
  let (tokens, _) = @html.tokenize("﷘")
  inspect(tokens, content="[Character('\\u{fdd8}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufdd9_6515" {
  let (tokens, _) = @html.tokenize("﷙")
  inspect(tokens, content="[Character('\\u{fdd9}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufdda_6516" {
  let (tokens, _) = @html.tokenize("﷚")
  inspect(tokens, content="[Character('\\u{fdda}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufddb_6517" {
  let (tokens, _) = @html.tokenize("﷛")
  inspect(tokens, content="[Character('\\u{fddb}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufddc_6518" {
  let (tokens, _) = @html.tokenize("﷜")
  inspect(tokens, content="[Character('\\u{fddc}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufddd_6519" {
  let (tokens, _) = @html.tokenize("﷝")
  inspect(tokens, content="[Character('\\u{fddd}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufdde_6520" {
  let (tokens, _) = @html.tokenize("﷞")
  inspect(tokens, content="[Character('\\u{fdde}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufddf_6521" {
  let (tokens, _) = @html.tokenize("﷟")
  inspect(tokens, content="[Character('\\u{fddf}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufde0_6522" {
  let (tokens, _) = @html.tokenize("﷠")
  inspect(tokens, content="[Character('\\u{fde0}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufde1_6523" {
  let (tokens, _) = @html.tokenize("﷡")
  inspect(tokens, content="[Character('\\u{fde1}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufde2_6524" {
  let (tokens, _) = @html.tokenize("﷢")
  inspect(tokens, content="[Character('\\u{fde2}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufde3_6525" {
  let (tokens, _) = @html.tokenize("﷣")
  inspect(tokens, content="[Character('\\u{fde3}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufde4_6526" {
  let (tokens, _) = @html.tokenize("﷤")
  inspect(tokens, content="[Character('\\u{fde4}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufde5_6527" {
  let (tokens, _) = @html.tokenize("﷥")
  inspect(tokens, content="[Character('\\u{fde5}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufde6_6528" {
  let (tokens, _) = @html.tokenize("﷦")
  inspect(tokens, content="[Character('\\u{fde6}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufde7_6529" {
  let (tokens, _) = @html.tokenize("﷧")
  inspect(tokens, content="[Character('\\u{fde7}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufde8_6530" {
  let (tokens, _) = @html.tokenize("﷨")
  inspect(tokens, content="[Character('\\u{fde8}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufde9_6531" {
  let (tokens, _) = @html.tokenize("﷩")
  inspect(tokens, content="[Character('\\u{fde9}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufdea_6532" {
  let (tokens, _) = @html.tokenize("﷪")
  inspect(tokens, content="[Character('\\u{fdea}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufdeb_6533" {
  let (tokens, _) = @html.tokenize("﷫")
  inspect(tokens, content="[Character('\\u{fdeb}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufdec_6534" {
  let (tokens, _) = @html.tokenize("﷬")
  inspect(tokens, content="[Character('\\u{fdec}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufded_6535" {
  let (tokens, _) = @html.tokenize("﷭")
  inspect(tokens, content="[Character('\\u{fded}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufdee_6536" {
  let (tokens, _) = @html.tokenize("﷮")
  inspect(tokens, content="[Character('\\u{fdee}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufdef_6537" {
  let (tokens, _) = @html.tokenize("﷯")
  inspect(tokens, content="[Character('\\u{fdef}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufffe_6538" {
  let (tokens, _) = @html.tokenize("￾")
  inspect(tokens, content="[Character('\\u{fffe}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_uffff_6539" {
  let (tokens, _) = @html.tokenize("￿")
  inspect(tokens, content="[Character('\\u{ffff}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u1fffe_6540" {
  let (tokens, _) = @html.tokenize("🿾")
  inspect(tokens, content="[Character('\\u{01fffe}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u1ffff_6541" {
  let (tokens, _) = @html.tokenize("🿿")
  inspect(tokens, content="[Character('\\u{01ffff}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u2fffe_6542" {
  let (tokens, _) = @html.tokenize("𯿾")
  inspect(tokens, content="[Character('\\u{02fffe}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u2ffff_6543" {
  let (tokens, _) = @html.tokenize("𯿿")
  inspect(tokens, content="[Character('\\u{02ffff}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u3fffe_6544" {
  let (tokens, _) = @html.tokenize("𿿾")
  inspect(tokens, content="[Character('\\u{03fffe}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u3ffff_6545" {
  let (tokens, _) = @html.tokenize("𿿿")
  inspect(tokens, content="[Character('\\u{03ffff}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u4fffe_6546" {
  let (tokens, _) = @html.tokenize("񏿾")
  inspect(tokens, content="[Character('\\u{04fffe}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u4ffff_6547" {
  let (tokens, _) = @html.tokenize("񏿿")
  inspect(tokens, content="[Character('\\u{04ffff}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u5fffe_6548" {
  let (tokens, _) = @html.tokenize("񟿾")
  inspect(tokens, content="[Character('\\u{05fffe}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u5ffff_6549" {
  let (tokens, _) = @html.tokenize("񟿿")
  inspect(tokens, content="[Character('\\u{05ffff}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u6fffe_6550" {
  let (tokens, _) = @html.tokenize("񯿾")
  inspect(tokens, content="[Character('\\u{06fffe}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u6ffff_6551" {
  let (tokens, _) = @html.tokenize("񯿿")
  inspect(tokens, content="[Character('\\u{06ffff}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u7fffe_6552" {
  let (tokens, _) = @html.tokenize("񿿾")
  inspect(tokens, content="[Character('\\u{07fffe}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u7ffff_6553" {
  let (tokens, _) = @html.tokenize("񿿿")
  inspect(tokens, content="[Character('\\u{07ffff}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u8fffe_6554" {
  let (tokens, _) = @html.tokenize("򏿾")
  inspect(tokens, content="[Character('\\u{08fffe}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u8ffff_6555" {
  let (tokens, _) = @html.tokenize("򏿿")
  inspect(tokens, content="[Character('\\u{08ffff}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u9fffe_6556" {
  let (tokens, _) = @html.tokenize("򟿾")
  inspect(tokens, content="[Character('\\u{09fffe}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u9ffff_6557" {
  let (tokens, _) = @html.tokenize("򟿿")
  inspect(tokens, content="[Character('\\u{09ffff}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_uafffe_6558" {
  let (tokens, _) = @html.tokenize("򯿾")
  inspect(tokens, content="[Character('\\u{0afffe}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_uaffff_6559" {
  let (tokens, _) = @html.tokenize("򯿿")
  inspect(tokens, content="[Character('\\u{0affff}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ubfffe_6560" {
  let (tokens, _) = @html.tokenize("򿿾")
  inspect(tokens, content="[Character('\\u{0bfffe}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ubffff_6561" {
  let (tokens, _) = @html.tokenize("򿿿")
  inspect(tokens, content="[Character('\\u{0bffff}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ucfffe_6562" {
  let (tokens, _) = @html.tokenize("󏿾")
  inspect(tokens, content="[Character('\\u{0cfffe}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ucffff_6563" {
  let (tokens, _) = @html.tokenize("󏿿")
  inspect(tokens, content="[Character('\\u{0cffff}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_udfffe_6564" {
  let (tokens, _) = @html.tokenize("󟿾")
  inspect(tokens, content="[Character('\\u{0dfffe}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_udffff_6565" {
  let (tokens, _) = @html.tokenize("󟿿")
  inspect(tokens, content="[Character('\\u{0dffff}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_uefffe_6566" {
  let (tokens, _) = @html.tokenize("󯿾")
  inspect(tokens, content="[Character('\\u{0efffe}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ueffff_6567" {
  let (tokens, _) = @html.tokenize("󯿿")
  inspect(tokens, content="[Character('\\u{0effff}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_uffffe_6568" {
  let (tokens, _) = @html.tokenize("󿿾")
  inspect(tokens, content="[Character('\\u{0ffffe}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_ufffff_6569" {
  let (tokens, _) = @html.tokenize("󿿿")
  inspect(tokens, content="[Character('\\u{0fffff}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u10fffe_6570" {
  let (tokens, _) = @html.tokenize("􏿾")
  inspect(tokens, content="[Character('\\u{10fffe}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_invalid_unicode_character_u10ffff_6571" {
  let (tokens, _) = @html.tokenize("􏿿")
  inspect(tokens, content="[Character('\\u{10ffff}'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0009_6572" {
  let (tokens, _) = @html.tokenize("\t")
  inspect(tokens, content="[Character('\\t'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u000a_6573" {
  let (tokens, _) = @html.tokenize("\n")
  inspect(tokens, content="[Character('\\n'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0020_6574" {
  let (tokens, _) = @html.tokenize(" ")
  inspect(tokens, content="[Character(' '), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0021_6575" {
  let (tokens, _) = @html.tokenize("!")
  inspect(tokens, content="[Character('!'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0022_6576" {
  let (tokens, _) = @html.tokenize("\"")
  inspect(tokens, content="[Character('\"'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0023_6577" {
  let (tokens, _) = @html.tokenize("#")
  inspect(tokens, content="[Character('#'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0024_6578" {
  let (tokens, _) = @html.tokenize("$")
  inspect(tokens, content="[Character('$'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0025_6579" {
  let (tokens, _) = @html.tokenize("%")
  inspect(tokens, content="[Character('%'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0026_6580" {
  let (tokens, _) = @html.tokenize("&")
  inspect(tokens, content="[Character('&'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0027_6581" {
  let (tokens, _) = @html.tokenize("'")
  inspect(tokens, content="[Character('\\''), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0028_6582" {
  let (tokens, _) = @html.tokenize("(")
  inspect(tokens, content="[Character('('), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0029_6583" {
  let (tokens, _) = @html.tokenize(")")
  inspect(tokens, content="[Character(')'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u002a_6584" {
  let (tokens, _) = @html.tokenize("*")
  inspect(tokens, content="[Character('*'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u002b_6585" {
  let (tokens, _) = @html.tokenize("+")
  inspect(tokens, content="[Character('+'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u002c_6586" {
  let (tokens, _) = @html.tokenize(",")
  inspect(tokens, content="[Character(','), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u002d_6587" {
  let (tokens, _) = @html.tokenize("-")
  inspect(tokens, content="[Character('-'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u002e_6588" {
  let (tokens, _) = @html.tokenize(".")
  inspect(tokens, content="[Character('.'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u002f_6589" {
  let (tokens, _) = @html.tokenize("/")
  inspect(tokens, content="[Character('/'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0030_6590" {
  let (tokens, _) = @html.tokenize("0")
  inspect(tokens, content="[Character('0'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0031_6591" {
  let (tokens, _) = @html.tokenize("1")
  inspect(tokens, content="[Character('1'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0032_6592" {
  let (tokens, _) = @html.tokenize("2")
  inspect(tokens, content="[Character('2'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0033_6593" {
  let (tokens, _) = @html.tokenize("3")
  inspect(tokens, content="[Character('3'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0034_6594" {
  let (tokens, _) = @html.tokenize("4")
  inspect(tokens, content="[Character('4'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0035_6595" {
  let (tokens, _) = @html.tokenize("5")
  inspect(tokens, content="[Character('5'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0036_6596" {
  let (tokens, _) = @html.tokenize("6")
  inspect(tokens, content="[Character('6'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0037_6597" {
  let (tokens, _) = @html.tokenize("7")
  inspect(tokens, content="[Character('7'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0038_6598" {
  let (tokens, _) = @html.tokenize("8")
  inspect(tokens, content="[Character('8'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0039_6599" {
  let (tokens, _) = @html.tokenize("9")
  inspect(tokens, content="[Character('9'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u003a_6600" {
  let (tokens, _) = @html.tokenize(":")
  inspect(tokens, content="[Character(':'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u003b_6601" {
  let (tokens, _) = @html.tokenize(";")
  inspect(tokens, content="[Character(';'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u003d_6602" {
  let (tokens, _) = @html.tokenize("=")
  inspect(tokens, content="[Character('='), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u003e_6603" {
  let (tokens, _) = @html.tokenize(">")
  inspect(tokens, content="[Character('>'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u003f_6604" {
  let (tokens, _) = @html.tokenize("?")
  inspect(tokens, content="[Character('?'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0040_6605" {
  let (tokens, _) = @html.tokenize("@")
  inspect(tokens, content="[Character('@'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0041_6606" {
  let (tokens, _) = @html.tokenize("A")
  inspect(tokens, content="[Character('A'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0042_6607" {
  let (tokens, _) = @html.tokenize("B")
  inspect(tokens, content="[Character('B'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0043_6608" {
  let (tokens, _) = @html.tokenize("C")
  inspect(tokens, content="[Character('C'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0044_6609" {
  let (tokens, _) = @html.tokenize("D")
  inspect(tokens, content="[Character('D'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0045_6610" {
  let (tokens, _) = @html.tokenize("E")
  inspect(tokens, content="[Character('E'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0046_6611" {
  let (tokens, _) = @html.tokenize("F")
  inspect(tokens, content="[Character('F'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0047_6612" {
  let (tokens, _) = @html.tokenize("G")
  inspect(tokens, content="[Character('G'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0048_6613" {
  let (tokens, _) = @html.tokenize("H")
  inspect(tokens, content="[Character('H'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0049_6614" {
  let (tokens, _) = @html.tokenize("I")
  inspect(tokens, content="[Character('I'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u004a_6615" {
  let (tokens, _) = @html.tokenize("J")
  inspect(tokens, content="[Character('J'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u004b_6616" {
  let (tokens, _) = @html.tokenize("K")
  inspect(tokens, content="[Character('K'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u004c_6617" {
  let (tokens, _) = @html.tokenize("L")
  inspect(tokens, content="[Character('L'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u004d_6618" {
  let (tokens, _) = @html.tokenize("M")
  inspect(tokens, content="[Character('M'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u004e_6619" {
  let (tokens, _) = @html.tokenize("N")
  inspect(tokens, content="[Character('N'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u004f_6620" {
  let (tokens, _) = @html.tokenize("O")
  inspect(tokens, content="[Character('O'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0050_6621" {
  let (tokens, _) = @html.tokenize("P")
  inspect(tokens, content="[Character('P'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0051_6622" {
  let (tokens, _) = @html.tokenize("Q")
  inspect(tokens, content="[Character('Q'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0052_6623" {
  let (tokens, _) = @html.tokenize("R")
  inspect(tokens, content="[Character('R'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0053_6624" {
  let (tokens, _) = @html.tokenize("S")
  inspect(tokens, content="[Character('S'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0054_6625" {
  let (tokens, _) = @html.tokenize("T")
  inspect(tokens, content="[Character('T'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0055_6626" {
  let (tokens, _) = @html.tokenize("U")
  inspect(tokens, content="[Character('U'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0056_6627" {
  let (tokens, _) = @html.tokenize("V")
  inspect(tokens, content="[Character('V'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0057_6628" {
  let (tokens, _) = @html.tokenize("W")
  inspect(tokens, content="[Character('W'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0058_6629" {
  let (tokens, _) = @html.tokenize("X")
  inspect(tokens, content="[Character('X'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0059_6630" {
  let (tokens, _) = @html.tokenize("Y")
  inspect(tokens, content="[Character('Y'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u005a_6631" {
  let (tokens, _) = @html.tokenize("Z")
  inspect(tokens, content="[Character('Z'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u005b_6632" {
  let (tokens, _) = @html.tokenize("[")
  inspect(tokens, content="[Character('['), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u005c_6633" {
  let (tokens, _) = @html.tokenize("\\")
  inspect(tokens, content="[Character('\\\\'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u005d_6634" {
  let (tokens, _) = @html.tokenize("]")
  inspect(tokens, content="[Character(']'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u005e_6635" {
  let (tokens, _) = @html.tokenize("^")
  inspect(tokens, content="[Character('^'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u005f_6636" {
  let (tokens, _) = @html.tokenize("_")
  inspect(tokens, content="[Character('_'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0060_6637" {
  let (tokens, _) = @html.tokenize("`")
  inspect(tokens, content="[Character('`'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0061_6638" {
  let (tokens, _) = @html.tokenize("a")
  inspect(tokens, content="[Character('a'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0062_6639" {
  let (tokens, _) = @html.tokenize("b")
  inspect(tokens, content="[Character('b'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0063_6640" {
  let (tokens, _) = @html.tokenize("c")
  inspect(tokens, content="[Character('c'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0064_6641" {
  let (tokens, _) = @html.tokenize("d")
  inspect(tokens, content="[Character('d'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0065_6642" {
  let (tokens, _) = @html.tokenize("e")
  inspect(tokens, content="[Character('e'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0066_6643" {
  let (tokens, _) = @html.tokenize("f")
  inspect(tokens, content="[Character('f'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0067_6644" {
  let (tokens, _) = @html.tokenize("g")
  inspect(tokens, content="[Character('g'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0068_6645" {
  let (tokens, _) = @html.tokenize("h")
  inspect(tokens, content="[Character('h'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0069_6646" {
  let (tokens, _) = @html.tokenize("i")
  inspect(tokens, content="[Character('i'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u006a_6647" {
  let (tokens, _) = @html.tokenize("j")
  inspect(tokens, content="[Character('j'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u006b_6648" {
  let (tokens, _) = @html.tokenize("k")
  inspect(tokens, content="[Character('k'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u006c_6649" {
  let (tokens, _) = @html.tokenize("l")
  inspect(tokens, content="[Character('l'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u006d_6650" {
  let (tokens, _) = @html.tokenize("m")
  inspect(tokens, content="[Character('m'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u006e_6651" {
  let (tokens, _) = @html.tokenize("n")
  inspect(tokens, content="[Character('n'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u006f_6652" {
  let (tokens, _) = @html.tokenize("o")
  inspect(tokens, content="[Character('o'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0070_6653" {
  let (tokens, _) = @html.tokenize("p")
  inspect(tokens, content="[Character('p'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0071_6654" {
  let (tokens, _) = @html.tokenize("q")
  inspect(tokens, content="[Character('q'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0072_6655" {
  let (tokens, _) = @html.tokenize("r")
  inspect(tokens, content="[Character('r'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0073_6656" {
  let (tokens, _) = @html.tokenize("s")
  inspect(tokens, content="[Character('s'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0074_6657" {
  let (tokens, _) = @html.tokenize("t")
  inspect(tokens, content="[Character('t'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0075_6658" {
  let (tokens, _) = @html.tokenize("u")
  inspect(tokens, content="[Character('u'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0076_6659" {
  let (tokens, _) = @html.tokenize("v")
  inspect(tokens, content="[Character('v'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0077_6660" {
  let (tokens, _) = @html.tokenize("w")
  inspect(tokens, content="[Character('w'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0078_6661" {
  let (tokens, _) = @html.tokenize("x")
  inspect(tokens, content="[Character('x'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u0079_6662" {
  let (tokens, _) = @html.tokenize("y")
  inspect(tokens, content="[Character('y'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u007a_6663" {
  let (tokens, _) = @html.tokenize("z")
  inspect(tokens, content="[Character('z'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u007b_6664" {
  let (tokens, _) = @html.tokenize("{")
  inspect(tokens, content="[Character('{'), EOF]")
}

///|
test "html5lib/tokenizer/unicodeChars_valid_unicode_character_u007c_6665" {
  let (tokens, _) = @html.tokenize("|")
  inspect(tokens, content="[Character('|'), EOF]")
}

// ============================================================================
// AUTO-GENERATED FILE - DO NOT MODIFY MANUALLY
// Generated by: scripts/generate_conformance_tests.py
// Regenerate with: python3 scripts/generate_conformance_tests.py
// ============================================================================

// Copyright 2025 International Digital Economy Academy
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

///|
/// html5lib Tokenizer Conformance Tests (continued)
/// Source: https://github.com/html5lib/html5lib-tests

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00a0_4555" {
  let (tokens, _) = @html.tokenize("&#x00a0;")
  inspect(tokens, content="[Character('¬†'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00a1_4556" {
  let (tokens, _) = @html.tokenize("&#x00a1;")
  inspect(tokens, content="[Character('¬°'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00a2_4557" {
  let (tokens, _) = @html.tokenize("&#x00a2;")
  inspect(tokens, content="[Character('¬¢'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00a3_4558" {
  let (tokens, _) = @html.tokenize("&#x00a3;")
  inspect(tokens, content="[Character('¬£'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00a4_4559" {
  let (tokens, _) = @html.tokenize("&#x00a4;")
  inspect(tokens, content="[Character('¬§'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00a5_4560" {
  let (tokens, _) = @html.tokenize("&#x00a5;")
  inspect(tokens, content="[Character('¬•'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00a6_4561" {
  let (tokens, _) = @html.tokenize("&#x00a6;")
  inspect(tokens, content="[Character('¬¶'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00a7_4562" {
  let (tokens, _) = @html.tokenize("&#x00a7;")
  inspect(tokens, content="[Character('¬ß'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00a8_4563" {
  let (tokens, _) = @html.tokenize("&#x00a8;")
  inspect(tokens, content="[Character('¬®'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00a9_4564" {
  let (tokens, _) = @html.tokenize("&#x00a9;")
  inspect(tokens, content="[Character('¬©'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00aa_4565" {
  let (tokens, _) = @html.tokenize("&#x00aa;")
  inspect(tokens, content="[Character('¬™'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00ab_4566" {
  let (tokens, _) = @html.tokenize("&#x00ab;")
  inspect(tokens, content="[Character('¬´'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00ac_4567" {
  let (tokens, _) = @html.tokenize("&#x00ac;")
  inspect(tokens, content="[Character('¬¨'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00ad_4568" {
  let (tokens, _) = @html.tokenize("&#x00ad;")
  inspect(tokens, content="[Character('¬≠'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00ae_4569" {
  let (tokens, _) = @html.tokenize("&#x00ae;")
  inspect(tokens, content="[Character('¬Æ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00af_4570" {
  let (tokens, _) = @html.tokenize("&#x00af;")
  inspect(tokens, content="[Character('¬Ø'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00b0_4571" {
  let (tokens, _) = @html.tokenize("&#x00b0;")
  inspect(tokens, content="[Character('¬∞'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00b1_4572" {
  let (tokens, _) = @html.tokenize("&#x00b1;")
  inspect(tokens, content="[Character('¬±'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00b2_4573" {
  let (tokens, _) = @html.tokenize("&#x00b2;")
  inspect(tokens, content="[Character('¬≤'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00b3_4574" {
  let (tokens, _) = @html.tokenize("&#x00b3;")
  inspect(tokens, content="[Character('¬≥'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00b4_4575" {
  let (tokens, _) = @html.tokenize("&#x00b4;")
  inspect(tokens, content="[Character('¬¥'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00b5_4576" {
  let (tokens, _) = @html.tokenize("&#x00b5;")
  inspect(tokens, content="[Character('¬µ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00b6_4577" {
  let (tokens, _) = @html.tokenize("&#x00b6;")
  inspect(tokens, content="[Character('¬∂'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00b7_4578" {
  let (tokens, _) = @html.tokenize("&#x00b7;")
  inspect(tokens, content="[Character('¬∑'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00b8_4579" {
  let (tokens, _) = @html.tokenize("&#x00b8;")
  inspect(tokens, content="[Character('¬∏'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00b9_4580" {
  let (tokens, _) = @html.tokenize("&#x00b9;")
  inspect(tokens, content="[Character('¬π'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00ba_4581" {
  let (tokens, _) = @html.tokenize("&#x00ba;")
  inspect(tokens, content="[Character('¬∫'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00bb_4582" {
  let (tokens, _) = @html.tokenize("&#x00bb;")
  inspect(tokens, content="[Character('¬ª'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00bc_4583" {
  let (tokens, _) = @html.tokenize("&#x00bc;")
  inspect(tokens, content="[Character('¬º'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00bd_4584" {
  let (tokens, _) = @html.tokenize("&#x00bd;")
  inspect(tokens, content="[Character('¬Ω'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00be_4585" {
  let (tokens, _) = @html.tokenize("&#x00be;")
  inspect(tokens, content="[Character('¬æ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00bf_4586" {
  let (tokens, _) = @html.tokenize("&#x00bf;")
  inspect(tokens, content="[Character('¬ø'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00c0_4587" {
  let (tokens, _) = @html.tokenize("&#x00c0;")
  inspect(tokens, content="[Character('√Ä'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00c1_4588" {
  let (tokens, _) = @html.tokenize("&#x00c1;")
  inspect(tokens, content="[Character('√Å'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00c2_4589" {
  let (tokens, _) = @html.tokenize("&#x00c2;")
  inspect(tokens, content="[Character('√Ç'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00c3_4590" {
  let (tokens, _) = @html.tokenize("&#x00c3;")
  inspect(tokens, content="[Character('√É'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00c4_4591" {
  let (tokens, _) = @html.tokenize("&#x00c4;")
  inspect(tokens, content="[Character('√Ñ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00c5_4592" {
  let (tokens, _) = @html.tokenize("&#x00c5;")
  inspect(tokens, content="[Character('√Ö'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00c6_4593" {
  let (tokens, _) = @html.tokenize("&#x00c6;")
  inspect(tokens, content="[Character('√Ü'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00c7_4594" {
  let (tokens, _) = @html.tokenize("&#x00c7;")
  inspect(tokens, content="[Character('√á'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00c8_4595" {
  let (tokens, _) = @html.tokenize("&#x00c8;")
  inspect(tokens, content="[Character('√à'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00c9_4596" {
  let (tokens, _) = @html.tokenize("&#x00c9;")
  inspect(tokens, content="[Character('√â'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00ca_4597" {
  let (tokens, _) = @html.tokenize("&#x00ca;")
  inspect(tokens, content="[Character('√ä'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00cb_4598" {
  let (tokens, _) = @html.tokenize("&#x00cb;")
  inspect(tokens, content="[Character('√ã'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00cc_4599" {
  let (tokens, _) = @html.tokenize("&#x00cc;")
  inspect(tokens, content="[Character('√å'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00cd_4600" {
  let (tokens, _) = @html.tokenize("&#x00cd;")
  inspect(tokens, content="[Character('√ç'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00ce_4601" {
  let (tokens, _) = @html.tokenize("&#x00ce;")
  inspect(tokens, content="[Character('√é'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00cf_4602" {
  let (tokens, _) = @html.tokenize("&#x00cf;")
  inspect(tokens, content="[Character('√è'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00d0_4603" {
  let (tokens, _) = @html.tokenize("&#x00d0;")
  inspect(tokens, content="[Character('√ê'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00d1_4604" {
  let (tokens, _) = @html.tokenize("&#x00d1;")
  inspect(tokens, content="[Character('√ë'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00d2_4605" {
  let (tokens, _) = @html.tokenize("&#x00d2;")
  inspect(tokens, content="[Character('√í'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00d3_4606" {
  let (tokens, _) = @html.tokenize("&#x00d3;")
  inspect(tokens, content="[Character('√ì'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00d4_4607" {
  let (tokens, _) = @html.tokenize("&#x00d4;")
  inspect(tokens, content="[Character('√î'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00d5_4608" {
  let (tokens, _) = @html.tokenize("&#x00d5;")
  inspect(tokens, content="[Character('√ï'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00d6_4609" {
  let (tokens, _) = @html.tokenize("&#x00d6;")
  inspect(tokens, content="[Character('√ñ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00d7_4610" {
  let (tokens, _) = @html.tokenize("&#x00d7;")
  inspect(tokens, content="[Character('√ó'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00d8_4611" {
  let (tokens, _) = @html.tokenize("&#x00d8;")
  inspect(tokens, content="[Character('√ò'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00d9_4612" {
  let (tokens, _) = @html.tokenize("&#x00d9;")
  inspect(tokens, content="[Character('√ô'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00da_4613" {
  let (tokens, _) = @html.tokenize("&#x00da;")
  inspect(tokens, content="[Character('√ö'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00db_4614" {
  let (tokens, _) = @html.tokenize("&#x00db;")
  inspect(tokens, content="[Character('√õ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00dc_4615" {
  let (tokens, _) = @html.tokenize("&#x00dc;")
  inspect(tokens, content="[Character('√ú'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00dd_4616" {
  let (tokens, _) = @html.tokenize("&#x00dd;")
  inspect(tokens, content="[Character('√ù'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00de_4617" {
  let (tokens, _) = @html.tokenize("&#x00de;")
  inspect(tokens, content="[Character('√û'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00df_4618" {
  let (tokens, _) = @html.tokenize("&#x00df;")
  inspect(tokens, content="[Character('√ü'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00e0_4619" {
  let (tokens, _) = @html.tokenize("&#x00e0;")
  inspect(tokens, content="[Character('√†'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00e1_4620" {
  let (tokens, _) = @html.tokenize("&#x00e1;")
  inspect(tokens, content="[Character('√°'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00e2_4621" {
  let (tokens, _) = @html.tokenize("&#x00e2;")
  inspect(tokens, content="[Character('√¢'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00e3_4622" {
  let (tokens, _) = @html.tokenize("&#x00e3;")
  inspect(tokens, content="[Character('√£'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00e4_4623" {
  let (tokens, _) = @html.tokenize("&#x00e4;")
  inspect(tokens, content="[Character('√§'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00e5_4624" {
  let (tokens, _) = @html.tokenize("&#x00e5;")
  inspect(tokens, content="[Character('√•'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00e6_4625" {
  let (tokens, _) = @html.tokenize("&#x00e6;")
  inspect(tokens, content="[Character('√¶'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00e7_4626" {
  let (tokens, _) = @html.tokenize("&#x00e7;")
  inspect(tokens, content="[Character('√ß'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00e8_4627" {
  let (tokens, _) = @html.tokenize("&#x00e8;")
  inspect(tokens, content="[Character('√®'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00e9_4628" {
  let (tokens, _) = @html.tokenize("&#x00e9;")
  inspect(tokens, content="[Character('√©'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00ea_4629" {
  let (tokens, _) = @html.tokenize("&#x00ea;")
  inspect(tokens, content="[Character('√™'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00eb_4630" {
  let (tokens, _) = @html.tokenize("&#x00eb;")
  inspect(tokens, content="[Character('√´'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00ec_4631" {
  let (tokens, _) = @html.tokenize("&#x00ec;")
  inspect(tokens, content="[Character('√¨'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00ed_4632" {
  let (tokens, _) = @html.tokenize("&#x00ed;")
  inspect(tokens, content="[Character('√≠'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00ee_4633" {
  let (tokens, _) = @html.tokenize("&#x00ee;")
  inspect(tokens, content="[Character('√Æ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00ef_4634" {
  let (tokens, _) = @html.tokenize("&#x00ef;")
  inspect(tokens, content="[Character('√Ø'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00f0_4635" {
  let (tokens, _) = @html.tokenize("&#x00f0;")
  inspect(tokens, content="[Character('√∞'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00f1_4636" {
  let (tokens, _) = @html.tokenize("&#x00f1;")
  inspect(tokens, content="[Character('√±'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00f2_4637" {
  let (tokens, _) = @html.tokenize("&#x00f2;")
  inspect(tokens, content="[Character('√≤'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00f3_4638" {
  let (tokens, _) = @html.tokenize("&#x00f3;")
  inspect(tokens, content="[Character('√≥'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00f4_4639" {
  let (tokens, _) = @html.tokenize("&#x00f4;")
  inspect(tokens, content="[Character('√¥'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00f5_4640" {
  let (tokens, _) = @html.tokenize("&#x00f5;")
  inspect(tokens, content="[Character('√µ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00f6_4641" {
  let (tokens, _) = @html.tokenize("&#x00f6;")
  inspect(tokens, content="[Character('√∂'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00f7_4642" {
  let (tokens, _) = @html.tokenize("&#x00f7;")
  inspect(tokens, content="[Character('√∑'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00f8_4643" {
  let (tokens, _) = @html.tokenize("&#x00f8;")
  inspect(tokens, content="[Character('√∏'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00f9_4644" {
  let (tokens, _) = @html.tokenize("&#x00f9;")
  inspect(tokens, content="[Character('√π'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00fa_4645" {
  let (tokens, _) = @html.tokenize("&#x00fa;")
  inspect(tokens, content="[Character('√∫'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00fb_4646" {
  let (tokens, _) = @html.tokenize("&#x00fb;")
  inspect(tokens, content="[Character('√ª'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00fc_4647" {
  let (tokens, _) = @html.tokenize("&#x00fc;")
  inspect(tokens, content="[Character('√º'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00fd_4648" {
  let (tokens, _) = @html.tokenize("&#x00fd;")
  inspect(tokens, content="[Character('√Ω'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00fe_4649" {
  let (tokens, _) = @html.tokenize("&#x00fe;")
  inspect(tokens, content="[Character('√æ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u00ff_4650" {
  let (tokens, _) = @html.tokenize("&#x00ff;")
  inspect(tokens, content="[Character('√ø'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_ud7ff_4651" {
  let (tokens, _) = @html.tokenize("&#xd7ff;")
  inspect(tokens, content="[Character('Ìüø'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_ue000_4652" {
  let (tokens, _) = @html.tokenize("&#xe000;")
  inspect(tokens, content="[Character('ÓÄÄ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_ufdcf_4653" {
  let (tokens, _) = @html.tokenize("&#xfdcf;")
  inspect(tokens, content="[Character('Ô∑è'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_ufdf0_4654" {
  let (tokens, _) = @html.tokenize("&#xfdf0;")
  inspect(tokens, content="[Character('Ô∑∞'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_ufffd_4655" {
  let (tokens, _) = @html.tokenize("&#xfffd;")
  inspect(tokens, content="[Character('ÔøΩ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u10000_4656" {
  let (tokens, _) = @html.tokenize("&#x10000;")
  inspect(tokens, content="[Character('êÄÄ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u1fffd_4657" {
  let (tokens, _) = @html.tokenize("&#x1fffd;")
  inspect(tokens, content="[Character('üøΩ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u20000_4658" {
  let (tokens, _) = @html.tokenize("&#x20000;")
  inspect(tokens, content="[Character('†ÄÄ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u2fffd_4659" {
  let (tokens, _) = @html.tokenize("&#x2fffd;")
  inspect(tokens, content="[Character('ØøΩ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u30000_4660" {
  let (tokens, _) = @html.tokenize("&#x30000;")
  inspect(tokens, content="[Character('∞ÄÄ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u3fffd_4661" {
  let (tokens, _) = @html.tokenize("&#x3fffd;")
  inspect(tokens, content="[Character('øøΩ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u40000_4662" {
  let (tokens, _) = @html.tokenize("&#x40000;")
  inspect(tokens, content="[Character('ÒÄÄÄ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u4fffd_4663" {
  let (tokens, _) = @html.tokenize("&#x4fffd;")
  inspect(tokens, content="[Character('ÒèøΩ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u50000_4664" {
  let (tokens, _) = @html.tokenize("&#x50000;")
  inspect(tokens, content="[Character('ÒêÄÄ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u5fffd_4665" {
  let (tokens, _) = @html.tokenize("&#x5fffd;")
  inspect(tokens, content="[Character('ÒüøΩ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u60000_4666" {
  let (tokens, _) = @html.tokenize("&#x60000;")
  inspect(tokens, content="[Character('Ò†ÄÄ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u6fffd_4667" {
  let (tokens, _) = @html.tokenize("&#x6fffd;")
  inspect(tokens, content="[Character('ÒØøΩ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u70000_4668" {
  let (tokens, _) = @html.tokenize("&#x70000;")
  inspect(tokens, content="[Character('Ò∞ÄÄ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u7fffd_4669" {
  let (tokens, _) = @html.tokenize("&#x7fffd;")
  inspect(tokens, content="[Character('ÒøøΩ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u80000_4670" {
  let (tokens, _) = @html.tokenize("&#x80000;")
  inspect(tokens, content="[Character('ÚÄÄÄ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u8fffd_4671" {
  let (tokens, _) = @html.tokenize("&#x8fffd;")
  inspect(tokens, content="[Character('ÚèøΩ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u90000_4672" {
  let (tokens, _) = @html.tokenize("&#x90000;")
  inspect(tokens, content="[Character('ÚêÄÄ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u9fffd_4673" {
  let (tokens, _) = @html.tokenize("&#x9fffd;")
  inspect(tokens, content="[Character('ÚüøΩ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_ua0000_4674" {
  let (tokens, _) = @html.tokenize("&#xa0000;")
  inspect(tokens, content="[Character('Ú†ÄÄ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_uafffd_4675" {
  let (tokens, _) = @html.tokenize("&#xafffd;")
  inspect(tokens, content="[Character('ÚØøΩ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_ub0000_4676" {
  let (tokens, _) = @html.tokenize("&#xb0000;")
  inspect(tokens, content="[Character('Ú∞ÄÄ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_ubfffd_4677" {
  let (tokens, _) = @html.tokenize("&#xbfffd;")
  inspect(tokens, content="[Character('ÚøøΩ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_uc0000_4678" {
  let (tokens, _) = @html.tokenize("&#xc0000;")
  inspect(tokens, content="[Character('ÛÄÄÄ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_ucfffd_4679" {
  let (tokens, _) = @html.tokenize("&#xcfffd;")
  inspect(tokens, content="[Character('ÛèøΩ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_ud0000_4680" {
  let (tokens, _) = @html.tokenize("&#xd0000;")
  inspect(tokens, content="[Character('ÛêÄÄ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_udfffd_4681" {
  let (tokens, _) = @html.tokenize("&#xdfffd;")
  inspect(tokens, content="[Character('ÛüøΩ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_ue0000_4682" {
  let (tokens, _) = @html.tokenize("&#xe0000;")
  inspect(tokens, content="[Character('Û†ÄÄ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_uefffd_4683" {
  let (tokens, _) = @html.tokenize("&#xefffd;")
  inspect(tokens, content="[Character('ÛØøΩ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_uf0000_4684" {
  let (tokens, _) = @html.tokenize("&#xf0000;")
  inspect(tokens, content="[Character('Û∞ÄÄ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_uffffd_4685" {
  let (tokens, _) = @html.tokenize("&#xffffd;")
  inspect(tokens, content="[Character('ÛøøΩ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u100000_4686" {
  let (tokens, _) = @html.tokenize("&#x100000;")
  inspect(tokens, content="[Character('ÙÄÄÄ'), EOF]")
}

///|
test "html5lib/tokenizer/numericEntities_valid_numeric_entity_character_u10fffd_4687" {
  let (tokens, _) = @html.tokenize("&#x10fffd;")
  inspect(tokens, content="[Character('ÙèøΩ'), EOF]")
}

///|
test "html5lib/tokenizer/pendingSpecChanges___4688" {
  let (tokens, _) = @html.tokenize("<!---- >")
  inspect(tokens, content="[Comment(\"-- >\"), EOF]")
}

///|
test "html5lib/tokenizer/test1_correct_doctype_lowercase_4689" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE html>")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"html\"), public_id=None, system_id=None, force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test1_correct_doctype_uppercase_4690" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE HTML>")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"html\"), public_id=None, system_id=None, force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test1_correct_doctype_mixed_case_4691" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE HtMl>")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"html\"), public_id=None, system_id=None, force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test1_correct_doctype_case_with_eof_4692" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE HtMl")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"html\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test1_truncated_doctype_start_4693" {
  let (tokens, _) = @html.tokenize("<!DOC>")
  inspect(tokens, content="[Comment(\"DOC\"), EOF]")
}

///|
test "html5lib/tokenizer/test1_doctype_in_error_4694" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE foo>")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"foo\"), public_id=None, system_id=None, force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test1_single_start_tag_4695" {
  let (tokens, _) = @html.tokenize("<h>")
  inspect(
    tokens,
    content="[StartTag(name=\"h\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test1_empty_end_tag_4696" {
  let (tokens, _) = @html.tokenize("</>")
  inspect(tokens, content="[EOF]")
}

///|
test "html5lib/tokenizer/test1_empty_start_tag_4697" {
  let (tokens, _) = @html.tokenize("<>")
  inspect(tokens, content="[Character('<'), Character('>'), EOF]")
}

///|
test "html5lib/tokenizer/test1_start_tag_wattribute_4698" {
  let (tokens, _) = @html.tokenize("<h a='b'>")
  inspect(
    tokens,
    content="[StartTag(name=\"h\", attrs=[{name: \"a\", value: \"b\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test1_start_tag_wattribute_no_quotes_4699" {
  let (tokens, _) = @html.tokenize("<h a=b>")
  inspect(
    tokens,
    content="[StartTag(name=\"h\", attrs=[{name: \"a\", value: \"b\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test1_startend_tag_4700" {
  let (tokens, _) = @html.tokenize("<h></h>")
  inspect(
    tokens,
    content="[StartTag(name=\"h\", attrs=[], self_closing=false), EndTag(name=\"h\"), EOF]",
  )
}

///|
test "html5lib/tokenizer/test1_two_unclosed_start_tags_4701" {
  let (tokens, _) = @html.tokenize("<p>One<p>Two")
  inspect(
    tokens,
    content="[StartTag(name=\"p\", attrs=[], self_closing=false), Character('O'), Character('n'), Character('e'), StartTag(name=\"p\", attrs=[], self_closing=false), Character('T'), Character('w'), Character('o'), EOF]",
  )
}

///|
test "html5lib/tokenizer/test1_end_tag_wattribute_4702" {
  let (tokens, _) = @html.tokenize("<h></h a='b'>")
  inspect(
    tokens,
    content="[StartTag(name=\"h\", attrs=[], self_closing=false), EndTag(name=\"h\"), EOF]",
  )
}

///|
test "html5lib/tokenizer/test1_multiple_atts_4703" {
  let (tokens, _) = @html.tokenize("<h a='b' c='d'>")
  inspect(
    tokens,
    content="[StartTag(name=\"h\", attrs=[{name: \"a\", value: \"b\"}, {name: \"c\", value: \"d\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test1_multiple_atts_no_space_4704" {
  let (tokens, _) = @html.tokenize("<h a='b'c='d'>")
  inspect(
    tokens,
    content="[StartTag(name=\"h\", attrs=[{name: \"a\", value: \"b\"}, {name: \"c\", value: \"d\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test1_repeated_attr_4705" {
  let (tokens, _) = @html.tokenize("<h a='b' a='d'>")
  inspect(
    tokens,
    content="[StartTag(name=\"h\", attrs=[{name: \"a\", value: \"b\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test1_simple_comment_4706" {
  let (tokens, _) = @html.tokenize("<!--comment-->")
  inspect(tokens, content="[Comment(\"comment\"), EOF]")
}

///|
test "html5lib/tokenizer/test1_comment_central_dash_no_space_4707" {
  let (tokens, _) = @html.tokenize("<!----->")
  inspect(tokens, content="[Comment(\"-\"), EOF]")
}

///|
test "html5lib/tokenizer/test1_comment_two_central_dashes_4708" {
  let (tokens, _) = @html.tokenize("<!-- --comment -->")
  inspect(tokens, content="[Comment(\" --comment \"), EOF]")
}

///|
test "html5lib/tokenizer/test1_comment_central_lessthan_bang_4709" {
  let (tokens, _) = @html.tokenize("<!--<!-->")
  inspect(tokens, content="[Comment(\"<!\"), EOF]")
}

///|
test "html5lib/tokenizer/test1_unfinished_comment_4710" {
  let (tokens, _) = @html.tokenize("<!--comment")
  inspect(tokens, content="[Comment(\"comment\"), EOF]")
}

///|
test "html5lib/tokenizer/test1_unfinished_comment_after_start_of_nested_comment_4711" {
  let (tokens, _) = @html.tokenize("<!-- <!--")
  inspect(tokens, content="[Comment(\" <!\"), EOF]")
}

///|
test "html5lib/tokenizer/test1_start_of_a_comment_4712" {
  let (tokens, _) = @html.tokenize("<!-")
  inspect(tokens, content="[Comment(\"-\"), EOF]")
}

///|
test "html5lib/tokenizer/test1_short_comment_4713" {
  let (tokens, _) = @html.tokenize("<!-->")
  inspect(tokens, content="[Comment(\"\"), EOF]")
}

///|
test "html5lib/tokenizer/test1_short_comment_two_4714" {
  let (tokens, _) = @html.tokenize("<!--->")
  inspect(tokens, content="[Comment(\"\"), EOF]")
}

///|
test "html5lib/tokenizer/test1_short_comment_three_4715" {
  let (tokens, _) = @html.tokenize("<!---->")
  inspect(tokens, content="[Comment(\"\"), EOF]")
}

///|
test "html5lib/tokenizer/test1__in_comment_4716" {
  let (tokens, _) = @html.tokenize("<!-- <test-->")
  inspect(tokens, content="[Comment(\" <test\"), EOF]")
}

///|
test "html5lib/tokenizer/test1__in_comment_4717" {
  let (tokens, _) = @html.tokenize("<!--<<-->")
  inspect(tokens, content="[Comment(\"<<\"), EOF]")
}

///|
test "html5lib/tokenizer/test1__in_comment_4718" {
  let (tokens, _) = @html.tokenize("<!-- <!test-->")
  inspect(tokens, content="[Comment(\" <!test\"), EOF]")
}

///|
test "html5lib/tokenizer/test1__in_comment_4719" {
  let (tokens, _) = @html.tokenize("<!-- <!-test-->")
  inspect(tokens, content="[Comment(\" <!-test\"), EOF]")
}

///|
test "html5lib/tokenizer/test1_nested_comment_4720" {
  let (tokens, _) = @html.tokenize("<!-- <!--test-->")
  inspect(tokens, content="[Comment(\" <!--test\"), EOF]")
}

///|
test "html5lib/tokenizer/test1_nested_comment_with_extra__4721" {
  let (tokens, _) = @html.tokenize("<!-- <<!--test-->")
  inspect(tokens, content="[Comment(\" <<!--test\"), EOF]")
}

///|
test "html5lib/tokenizer/test1_ampersand_eof_4735" {
  let (tokens, _) = @html.tokenize("&")
  inspect(tokens, content="[Character('&'), EOF]")
}

///|
test "html5lib/tokenizer/test1_ampersand_ampersand_eof_4736" {
  let (tokens, _) = @html.tokenize("&&")
  inspect(tokens, content="[Character('&'), Character('&'), EOF]")
}

///|
test "html5lib/tokenizer/test1_ampersand_space_eof_4737" {
  let (tokens, _) = @html.tokenize("& ")
  inspect(tokens, content="[Character('&'), Character(' '), EOF]")
}

///|
test "html5lib/tokenizer/test1_unfinished_entity_4738" {
  let (tokens, _) = @html.tokenize("&f")
  inspect(tokens, content="[Character('&'), Character('f'), EOF]")
}

///|
test "html5lib/tokenizer/test1_ampersand_number_sign_4739" {
  let (tokens, _) = @html.tokenize("&#")
  inspect(tokens, content="[Character('&'), Character('#'), EOF]")
}

///|
test "html5lib/tokenizer/test1_unfinished_numeric_entity_4740" {
  let (tokens, _) = @html.tokenize("&#x")
  inspect(
    tokens,
    content="[Character('&'), Character('#'), Character('x'), EOF]",
  )
}

///|
test "html5lib/tokenizer/test1_entity_with_trailing_semicolon_1_4741" {
  let (tokens, _) = @html.tokenize("I'm &not;it")
  inspect(
    tokens,
    content="[Character('I'), Character('\\''), Character('m'), Character(' '), Character('¬¨'), Character('i'), Character('t'), EOF]",
  )
}

///|
test "html5lib/tokenizer/test1_entity_with_trailing_semicolon_2_4742" {
  let (tokens, _) = @html.tokenize("I'm &notin;")
  inspect(
    tokens,
    content="[Character('I'), Character('\\''), Character('m'), Character(' '), Character('‚àâ'), EOF]",
  )
}

///|
test "html5lib/tokenizer/test1_entity_without_trailing_semicolon_1_4743" {
  let (tokens, _) = @html.tokenize("I'm &notit")
  inspect(
    tokens,
    content="[Character('I'), Character('\\''), Character('m'), Character(' '), Character('¬¨'), Character('i'), Character('t'), EOF]",
  )
}

///|
test "html5lib/tokenizer/test1_entity_without_trailing_semicolon_2_4744" {
  let (tokens, _) = @html.tokenize("I'm &notin")
  inspect(
    tokens,
    content="[Character('I'), Character('\\''), Character('m'), Character(' '), Character('¬¨'), Character('i'), Character('n'), EOF]",
  )
}

///|
test "html5lib/tokenizer/test1_partial_entity_match_at_end_of_file_4745" {
  let (tokens, _) = @html.tokenize("I'm &no")
  inspect(
    tokens,
    content="[Character('I'), Character('\\''), Character('m'), Character(' '), Character('&'), Character('n'), Character('o'), EOF]",
  )
}

///|
test "html5lib/tokenizer/test1_nonascii_character_reference_name_4746" {
  let (tokens, _) = @html.tokenize("&¬¨;")
  inspect(
    tokens,
    content="[Character('&'), Character('¬¨'), Character(';'), EOF]",
  )
}

///|
test "html5lib/tokenizer/test1_ascii_decimal_entity_4747" {
  let (tokens, _) = @html.tokenize("&#0036;")
  inspect(tokens, content="[Character('$'), EOF]")
}

///|
test "html5lib/tokenizer/test1_ascii_hexadecimal_entity_4748" {
  let (tokens, _) = @html.tokenize("&#x3f;")
  inspect(tokens, content="[Character('?'), EOF]")
}

///|
test "html5lib/tokenizer/test1_hexadecimal_entity_in_attribute_4749" {
  let (tokens, _) = @html.tokenize("<h a='&#x3f;'></h>")
  inspect(
    tokens,
    content="[StartTag(name=\"h\", attrs=[{name: \"a\", value: \"?\"}], self_closing=false), EndTag(name=\"h\"), EOF]",
  )
}

///|
test "html5lib/tokenizer/test1_entity_in_attribute_without_semicolon_ending_in_x_4750" {
  let (tokens, _) = @html.tokenize("<h a='&notx'>")
  inspect(
    tokens,
    content="[StartTag(name=\"h\", attrs=[{name: \"a\", value: \"&notx\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test1_entity_in_attribute_without_semicolon_ending_in_1_4751" {
  let (tokens, _) = @html.tokenize("<h a='&not1'>")
  inspect(
    tokens,
    content="[StartTag(name=\"h\", attrs=[{name: \"a\", value: \"&not1\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test1_entity_in_attribute_without_semicolon_ending_in_i_4752" {
  let (tokens, _) = @html.tokenize("<h a='&noti'>")
  inspect(
    tokens,
    content="[StartTag(name=\"h\", attrs=[{name: \"a\", value: \"&noti\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test1_entity_in_attribute_without_semicolon_4753" {
  let (tokens, _) = @html.tokenize("<h a='&COPY'>")
  inspect(
    tokens,
    content="[StartTag(name=\"h\", attrs=[{name: \"a\", value: \"¬©\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test1_unquoted_attribute_ending_in_ampersand_4754" {
  let (tokens, _) = @html.tokenize("<s o=& t>")
  inspect(
    tokens,
    content="[StartTag(name=\"s\", attrs=[{name: \"o\", value: \"&\"}, {name: \"t\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test1_unquoted_attribute_at_end_of_tag_with_final_charac_4755" {
  let (tokens, _) = @html.tokenize("<a a=a&>foo")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"a&\"}], self_closing=false), Character('f'), Character('o'), Character('o'), EOF]",
  )
}

///|
test "html5lib/tokenizer/test1_plaintext_element_4756" {
  let (tokens, _) = @html.tokenize("<plaintext>foobar")
  inspect(
    tokens,
    content="[StartTag(name=\"plaintext\", attrs=[], self_closing=false), Character('f'), Character('o'), Character('o'), Character('b'), Character('a'), Character('r'), EOF]",
  )
}

///|
test "html5lib/tokenizer/test1_open_angled_bracket_in_unquoted_attribute_value_st_4757" {
  let (tokens, _) = @html.tokenize("<a a=f<>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"f<\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test2_doctype_without_name_4758" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE>")
  inspect(
    tokens,
    content="[DOCTYPE(name=None, public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test2_doctype_without_space_before_name_4759" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEhtml>")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"html\"), public_id=None, system_id=None, force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test2_incorrect_doctype_without_a_space_before_name_4760" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEfoo>")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"foo\"), public_id=None, system_id=None, force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test2_doctype_with_publicid_4761" {
  let (tokens, _) = @html.tokenize(
    "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML Transitional 4.01//EN\">",
  )
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"html\"), public_id=Some(\"-//W3C//DTD HTML Transitional 4.01//EN\"), system_id=None, force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test2_doctype_with_eof_after_public_4762" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE html PUBLIC")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"html\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test2_doctype_with_eof_after_public__4763" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE html PUBLIC '")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"html\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test2_doctype_with_eof_after_public_x_4764" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE html PUBLIC 'x")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"html\"), public_id=Some(\"x\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test2_doctype_with_systemid_4765" {
  let (tokens, _) = @html.tokenize(
    "<!DOCTYPE html SYSTEM \"-//W3C//DTD HTML Transitional 4.01//EN\">",
  )
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"html\"), public_id=None, system_id=Some(\"-//W3C//DTD HTML Transitional 4.01//EN\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test2_doctype_with_singlequoted_systemid_4766" {
  let (tokens, _) = @html.tokenize(
    "<!DOCTYPE html SYSTEM '-//W3C//DTD HTML Transitional 4.01//EN'>",
  )
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"html\"), public_id=None, system_id=Some(\"-//W3C//DTD HTML Transitional 4.01//EN\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test2_doctype_with_publicid_and_systemid_4767" {
  let (tokens, _) = @html.tokenize(
    "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML Transitional 4.01//EN\" \"-//W3C//DTD HTML Transitional 4.01//EN\">",
  )
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"html\"), public_id=Some(\"-//W3C//DTD HTML Transitional 4.01//EN\"), system_id=Some(\"-//W3C//DTD HTML Transitional 4.01//EN\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test2_doctype_with_in_doublequoted_publicid_4768" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE html PUBLIC \">x")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"html\"), public_id=None, system_id=None, force_quirks=true), Character('x'), EOF]",
  )
}

///|
test "html5lib/tokenizer/test2_doctype_with_in_singlequoted_publicid_4769" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE html PUBLIC '>x")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"html\"), public_id=None, system_id=None, force_quirks=true), Character('x'), EOF]",
  )
}

///|
test "html5lib/tokenizer/test2_doctype_with_in_doublequoted_systemid_4770" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE html PUBLIC \"foo\" \">x")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"html\"), public_id=Some(\"foo\"), system_id=None, force_quirks=true), Character('x'), EOF]",
  )
}

///|
test "html5lib/tokenizer/test2_doctype_with_in_singlequoted_systemid_4771" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE html PUBLIC 'foo' '>x")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"html\"), public_id=Some(\"foo\"), system_id=None, force_quirks=true), Character('x'), EOF]",
  )
}

///|
test "html5lib/tokenizer/test2_incomplete_doctype_4772" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE html ")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"html\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test2_numeric_entity_representing_the_nul_character_4773" {
  let (tokens, _) = @html.tokenize("&#0000;")
  inspect(tokens, content="[Character('ÔøΩ'), EOF]")
}

///|
test "html5lib/tokenizer/test2_hexadecimal_entity_representing_the_nul_character_4774" {
  let (tokens, _) = @html.tokenize("&#x0000;")
  inspect(tokens, content="[Character('ÔøΩ'), EOF]")
}

///|
test "html5lib/tokenizer/test2_numeric_entity_representing_a_codepoint_after_1114_4775" {
  let (tokens, _) = @html.tokenize("&#2225222;")
  inspect(tokens, content="[Character('ÔøΩ'), EOF]")
}

///|
test "html5lib/tokenizer/test2_hexadecimal_entity_representing_a_codepoint_after__4776" {
  let (tokens, _) = @html.tokenize("&#x1010FFFF;")
  inspect(tokens, content="[Character('ÔøΩ'), EOF]")
}

///|
test "html5lib/tokenizer/test2_hexadecimal_entity_pair_representing_a_surrogate_p_4777" {
  let (tokens, _) = @html.tokenize("&#xD869;&#xDED6;")
  inspect(tokens, content="[Character('ÔøΩ'), Character('ÔøΩ'), EOF]")
}

///|
test "html5lib/tokenizer/test2_hexadecimal_entity_with_mixed_uppercase_and_lowerc_4778" {
  let (tokens, _) = @html.tokenize("&#xaBcD;")
  inspect(tokens, content="[Character('ÍØç'), EOF]")
}

///|
test "html5lib/tokenizer/test2_entity_without_a_name_4779" {
  let (tokens, _) = @html.tokenize("&;")
  inspect(tokens, content="[Character('&'), Character(';'), EOF]")
}

///|
test "html5lib/tokenizer/test2_unescaped_ampersand_in_attribute_value_4780" {
  let (tokens, _) = @html.tokenize("<h a='&'>")
  inspect(
    tokens,
    content="[StartTag(name=\"h\", attrs=[{name: \"a\", value: \"&\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test2_starttag_containing__4781" {
  let (tokens, _) = @html.tokenize("<a<b>")
  inspect(
    tokens,
    content="[StartTag(name=\"a<b\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test2_nonvoid_element_containing_trailing__4782" {
  let (tokens, _) = @html.tokenize("<h/>")
  inspect(
    tokens,
    content="[StartTag(name=\"h\", attrs=[], self_closing=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test2_void_element_with_permitted_slash_4783" {
  let (tokens, _) = @html.tokenize("<br/>")
  inspect(
    tokens,
    content="[StartTag(name=\"br\", attrs=[], self_closing=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test2_void_element_with_permitted_slash_with_attribute_4784" {
  let (tokens, _) = @html.tokenize("<br foo='bar'/>")
  inspect(
    tokens,
    content="[StartTag(name=\"br\", attrs=[{name: \"foo\", value: \"bar\"}], self_closing=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test2_starttag_containing__4785" {
  let (tokens, _) = @html.tokenize("<h/a='b'>")
  inspect(
    tokens,
    content="[StartTag(name=\"h\", attrs=[{name: \"a\", value: \"b\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test2_doublequoted_attribute_value_4786" {
  let (tokens, _) = @html.tokenize("<h a=\"b\">")
  inspect(
    tokens,
    content="[StartTag(name=\"h\", attrs=[{name: \"a\", value: \"b\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test2_unescaped__4787" {
  let (tokens, _) = @html.tokenize("</")
  inspect(tokens, content="[Character('<'), Character('/'), EOF]")
}

///|
test "html5lib/tokenizer/test2_illegal_end_tag_name_4788" {
  let (tokens, _) = @html.tokenize("</1>")
  inspect(tokens, content="[Comment(\"1\"), EOF]")
}

///|
test "html5lib/tokenizer/test2_simili_processing_instruction_4789" {
  let (tokens, _) = @html.tokenize("<?namespace>")
  inspect(tokens, content="[Comment(\"?namespace\"), EOF]")
}

///|
test "html5lib/tokenizer/test2_a_bogus_comment_stops_at_even_if_preceded_by_two_d_4790" {
  let (tokens, _) = @html.tokenize("<?foo-->")
  inspect(tokens, content="[Comment(\"?foo--\"), EOF]")
}

///|
test "html5lib/tokenizer/test2_unescaped__4791" {
  let (tokens, _) = @html.tokenize("foo < bar")
  inspect(
    tokens,
    content="[Character('f'), Character('o'), Character('o'), Character(' '), Character('<'), Character(' '), Character('b'), Character('a'), Character('r'), EOF]",
  )
}

///|
test "html5lib/tokenizer/test2_null_byte_replacement_4792" {
  let (tokens, _) = @html.tokenize("\u{0}")
  inspect(tokens, content="[Character('\\u{0}'), EOF]")
}

///|
test "html5lib/tokenizer/test2_comment_with_dash_4793" {
  let (tokens, _) = @html.tokenize("<!---x")
  inspect(tokens, content="[Comment(\"-x\"), EOF]")
}

///|
test "html5lib/tokenizer/test2_entity_newline_4794" {
  let (tokens, _) = @html.tokenize("\nx\n&gt;\n")
  inspect(
    tokens,
    content="[Character('\\n'), Character('x'), Character('\\n'), Character('>'), Character('\\n'), EOF]",
  )
}

///|
test "html5lib/tokenizer/test2_start_tag_with_no_attributes_but_space_before_the__4795" {
  let (tokens, _) = @html.tokenize("<h >")
  inspect(
    tokens,
    content="[StartTag(name=\"h\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test2_empty_attribute_followed_by_uppercase_attribute_4796" {
  let (tokens, _) = @html.tokenize("<h a B=''>")
  inspect(
    tokens,
    content="[StartTag(name=\"h\", attrs=[{name: \"a\", value: \"\"}, {name: \"b\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test2_doublequote_after_attribute_name_4797" {
  let (tokens, _) = @html.tokenize("<h a \">")
  inspect(
    tokens,
    content="[StartTag(name=\"h\", attrs=[{name: \"a\", value: \"\"}, {name: \"\\\"\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test2_singlequote_after_attribute_name_4798" {
  let (tokens, _) = @html.tokenize("<h a '>")
  inspect(
    tokens,
    content="[StartTag(name=\"h\", attrs=[{name: \"a\", value: \"\"}, {name: \"'\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test2_empty_end_tag_with_following_characters_4799" {
  let (tokens, _) = @html.tokenize("a</>bc")
  inspect(
    tokens,
    content="[Character('a'), Character('b'), Character('c'), EOF]",
  )
}

///|
test "html5lib/tokenizer/test2_empty_end_tag_with_following_tag_4800" {
  let (tokens, _) = @html.tokenize("a</><b>c")
  inspect(
    tokens,
    content="[Character('a'), StartTag(name=\"b\", attrs=[], self_closing=false), Character('c'), EOF]",
  )
}

///|
test "html5lib/tokenizer/test2_empty_end_tag_with_following_comment_4801" {
  let (tokens, _) = @html.tokenize("a</><!--b-->c")
  inspect(
    tokens,
    content="[Character('a'), Comment(\"b\"), Character('c'), EOF]",
  )
}

///|
test "html5lib/tokenizer/test2_empty_end_tag_with_following_end_tag_4802" {
  let (tokens, _) = @html.tokenize("a</></b>c")
  inspect(
    tokens,
    content="[Character('a'), EndTag(name=\"b\"), Character('c'), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3__4871" {
  let (tokens, _) = @html.tokenize("<")
  inspect(tokens, content="[Character('<'), EOF]")
}

///|
test "html5lib/tokenizer/test3_u0000_4872" {
  let (tokens, _) = @html.tokenize("<\u{0}")
  inspect(tokens, content="[Character('<'), Character('\\u{0}'), EOF]")
}

///|
test "html5lib/tokenizer/test3_u0009_4873" {
  let (tokens, _) = @html.tokenize("<\t")
  inspect(tokens, content="[Character('<'), Character('\\t'), EOF]")
}

///|
test "html5lib/tokenizer/test3_u000a_4874" {
  let (tokens, _) = @html.tokenize("<\n")
  inspect(tokens, content="[Character('<'), Character('\\n'), EOF]")
}

///|
test "html5lib/tokenizer/test3_u000b_4875" {
  let (tokens, _) = @html.tokenize("<\u{b}")
  inspect(tokens, content="[Character('<'), Character('\\u{b}'), EOF]")
}

///|
test "html5lib/tokenizer/test3_u000c_4876" {
  let (tokens, _) = @html.tokenize("<\u{c}")
  inspect(tokens, content="[Character('<'), Character('\\u{c}'), EOF]")
}

///|
test "html5lib/tokenizer/test3___4877" {
  let (tokens, _) = @html.tokenize("< ")
  inspect(tokens, content="[Character('<'), Character(' '), EOF]")
}

///|
test "html5lib/tokenizer/test3__4878" {
  let (tokens, _) = @html.tokenize("<!")
  inspect(tokens, content="[Comment(\"\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_u0000_4879" {
  let (tokens, _) = @html.tokenize("<!\u{0}")
  inspect(tokens, content="[Comment(\"ÔøΩ\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_u0009_4880" {
  let (tokens, _) = @html.tokenize("<!\t")
  inspect(tokens, content="[Comment(\"\\t\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_u000a_4881" {
  let (tokens, _) = @html.tokenize("<!\n")
  inspect(tokens, content="[Comment(\"\\n\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_u000b_4882" {
  let (tokens, _) = @html.tokenize("<!\u{b}")
  inspect(tokens, content="[Comment(\"\\u{b}\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_u000c_4883" {
  let (tokens, _) = @html.tokenize("<!\u{c}")
  inspect(tokens, content="[Comment(\"\\u{c}\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4884" {
  let (tokens, _) = @html.tokenize("<! ")
  inspect(tokens, content="[Comment(\" \"), EOF]")
}

///|
test "html5lib/tokenizer/test3__u0000_4885" {
  let (tokens, _) = @html.tokenize("<! \u{0}")
  inspect(tokens, content="[Comment(\" ÔøΩ\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__4886" {
  let (tokens, _) = @html.tokenize("<!!")
  inspect(tokens, content="[Comment(\"!\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__4887" {
  let (tokens, _) = @html.tokenize("<!\"")
  inspect(tokens, content="[Comment(\"\\\"\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__4888" {
  let (tokens, _) = @html.tokenize("<!&")
  inspect(tokens, content="[Comment(\"&\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__4889" {
  let (tokens, _) = @html.tokenize("<!'")
  inspect(tokens, content="[Comment(\"'\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__4890" {
  let (tokens, _) = @html.tokenize("<!-")
  inspect(tokens, content="[Comment(\"-\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__4891" {
  let (tokens, _) = @html.tokenize("<!--")
  inspect(tokens, content="[Comment(\"\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_u0000_4892" {
  let (tokens, _) = @html.tokenize("<!--\u{0}")
  inspect(tokens, content="[Comment(\"ÔøΩ\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_u0009_4893" {
  let (tokens, _) = @html.tokenize("<!--\t")
  inspect(tokens, content="[Comment(\"\\t\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_u000a_4894" {
  let (tokens, _) = @html.tokenize("<!--\n")
  inspect(tokens, content="[Comment(\"\\n\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_u000b_4895" {
  let (tokens, _) = @html.tokenize("<!--\u{b}")
  inspect(tokens, content="[Comment(\"\\u{b}\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_u000c_4896" {
  let (tokens, _) = @html.tokenize("<!--\u{c}")
  inspect(tokens, content="[Comment(\"\\u{c}\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4897" {
  let (tokens, _) = @html.tokenize("<!-- ")
  inspect(tokens, content="[Comment(\" \"), EOF]")
}

///|
test "html5lib/tokenizer/test3__u0000_4898" {
  let (tokens, _) = @html.tokenize("<!-- \u{0}")
  inspect(tokens, content="[Comment(\" ÔøΩ\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__u0009_4899" {
  let (tokens, _) = @html.tokenize("<!-- \t")
  inspect(tokens, content="[Comment(\" \\t\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__u000a_4900" {
  let (tokens, _) = @html.tokenize("<!-- \n")
  inspect(tokens, content="[Comment(\" \\n\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__u000b_4901" {
  let (tokens, _) = @html.tokenize("<!-- \u{b}")
  inspect(tokens, content="[Comment(\" \\u{b}\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__u000c_4902" {
  let (tokens, _) = @html.tokenize("<!-- \u{c}")
  inspect(tokens, content="[Comment(\" \\u{c}\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4903" {
  let (tokens, _) = @html.tokenize("<!--  ")
  inspect(tokens, content="[Comment(\"  \"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4904" {
  let (tokens, _) = @html.tokenize("<!-- !")
  inspect(tokens, content="[Comment(\" !\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4905" {
  let (tokens, _) = @html.tokenize("<!-- \"")
  inspect(tokens, content="[Comment(\" \\\"\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4906" {
  let (tokens, _) = @html.tokenize("<!-- &")
  inspect(tokens, content="[Comment(\" &\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4907" {
  let (tokens, _) = @html.tokenize("<!-- '")
  inspect(tokens, content="[Comment(\" '\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4908" {
  let (tokens, _) = @html.tokenize("<!-- ,")
  inspect(tokens, content="[Comment(\" ,\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4909" {
  let (tokens, _) = @html.tokenize("<!-- -")
  inspect(tokens, content="[Comment(\" \"), EOF]")
}

///|
test "html5lib/tokenizer/test3__u0000_4910" {
  let (tokens, _) = @html.tokenize("<!-- -\u{0}")
  inspect(tokens, content="[Comment(\" -ÔøΩ\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__u0009_4911" {
  let (tokens, _) = @html.tokenize("<!-- -\t")
  inspect(tokens, content="[Comment(\" -\\t\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__u000a_4912" {
  let (tokens, _) = @html.tokenize("<!-- -\n")
  inspect(tokens, content="[Comment(\" -\\n\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__u000b_4913" {
  let (tokens, _) = @html.tokenize("<!-- -\u{b}")
  inspect(tokens, content="[Comment(\" -\\u{b}\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__u000c_4914" {
  let (tokens, _) = @html.tokenize("<!-- -\u{c}")
  inspect(tokens, content="[Comment(\" -\\u{c}\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4915" {
  let (tokens, _) = @html.tokenize("<!-- - ")
  inspect(tokens, content="[Comment(\" - \"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4916" {
  let (tokens, _) = @html.tokenize("<!-- -!")
  inspect(tokens, content="[Comment(\" -!\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4917" {
  let (tokens, _) = @html.tokenize("<!-- -\"")
  inspect(tokens, content="[Comment(\" -\\\"\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4918" {
  let (tokens, _) = @html.tokenize("<!-- -&")
  inspect(tokens, content="[Comment(\" -&\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4919" {
  let (tokens, _) = @html.tokenize("<!-- -'")
  inspect(tokens, content="[Comment(\" -'\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4920" {
  let (tokens, _) = @html.tokenize("<!-- -,")
  inspect(tokens, content="[Comment(\" -,\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4921" {
  let (tokens, _) = @html.tokenize("<!-- --")
  inspect(tokens, content="[Comment(\" \"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4922" {
  let (tokens, _) = @html.tokenize("<!-- -.")
  inspect(tokens, content="[Comment(\" -.\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4923" {
  let (tokens, _) = @html.tokenize("<!-- -/")
  inspect(tokens, content="[Comment(\" -/\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__0_4924" {
  let (tokens, _) = @html.tokenize("<!-- -0")
  inspect(tokens, content="[Comment(\" -0\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__1_4925" {
  let (tokens, _) = @html.tokenize("<!-- -1")
  inspect(tokens, content="[Comment(\" -1\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__9_4926" {
  let (tokens, _) = @html.tokenize("<!-- -9")
  inspect(tokens, content="[Comment(\" -9\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4927" {
  let (tokens, _) = @html.tokenize("<!-- -<")
  inspect(tokens, content="[Comment(\" -<\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4928" {
  let (tokens, _) = @html.tokenize("<!-- -=")
  inspect(tokens, content="[Comment(\" -=\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4929" {
  let (tokens, _) = @html.tokenize("<!-- ->")
  inspect(tokens, content="[Comment(\" ->\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4930" {
  let (tokens, _) = @html.tokenize("<!-- -?")
  inspect(tokens, content="[Comment(\" -?\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4931" {
  let (tokens, _) = @html.tokenize("<!-- -@")
  inspect(tokens, content="[Comment(\" -@\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__a_4932" {
  let (tokens, _) = @html.tokenize("<!-- -A")
  inspect(tokens, content="[Comment(\" -A\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__b_4933" {
  let (tokens, _) = @html.tokenize("<!-- -B")
  inspect(tokens, content="[Comment(\" -B\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__y_4934" {
  let (tokens, _) = @html.tokenize("<!-- -Y")
  inspect(tokens, content="[Comment(\" -Y\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__z_4935" {
  let (tokens, _) = @html.tokenize("<!-- -Z")
  inspect(tokens, content="[Comment(\" -Z\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4936" {
  let (tokens, _) = @html.tokenize("<!-- -`")
  inspect(tokens, content="[Comment(\" -`\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__a_4937" {
  let (tokens, _) = @html.tokenize("<!-- -a")
  inspect(tokens, content="[Comment(\" -a\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__b_4938" {
  let (tokens, _) = @html.tokenize("<!-- -b")
  inspect(tokens, content="[Comment(\" -b\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__y_4939" {
  let (tokens, _) = @html.tokenize("<!-- -y")
  inspect(tokens, content="[Comment(\" -y\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__z_4940" {
  let (tokens, _) = @html.tokenize("<!-- -z")
  inspect(tokens, content="[Comment(\" -z\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4941" {
  let (tokens, _) = @html.tokenize("<!-- -{")
  inspect(tokens, content="[Comment(\" -{\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__udbc0udc00_4942" {
  let (tokens, _) = @html.tokenize("<!-- -ÙÄÄÄ")
  inspect(tokens, content="[Comment(\" -ÙÄÄÄ\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4943" {
  let (tokens, _) = @html.tokenize("<!-- .")
  inspect(tokens, content="[Comment(\" .\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4944" {
  let (tokens, _) = @html.tokenize("<!-- /")
  inspect(tokens, content="[Comment(\" /\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__0_4945" {
  let (tokens, _) = @html.tokenize("<!-- 0")
  inspect(tokens, content="[Comment(\" 0\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__1_4946" {
  let (tokens, _) = @html.tokenize("<!-- 1")
  inspect(tokens, content="[Comment(\" 1\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__9_4947" {
  let (tokens, _) = @html.tokenize("<!-- 9")
  inspect(tokens, content="[Comment(\" 9\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4948" {
  let (tokens, _) = @html.tokenize("<!-- <")
  inspect(tokens, content="[Comment(\" <\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4949" {
  let (tokens, _) = @html.tokenize("<!-- =")
  inspect(tokens, content="[Comment(\" =\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4950" {
  let (tokens, _) = @html.tokenize("<!-- >")
  inspect(tokens, content="[Comment(\" >\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4951" {
  let (tokens, _) = @html.tokenize("<!-- ?")
  inspect(tokens, content="[Comment(\" ?\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4952" {
  let (tokens, _) = @html.tokenize("<!-- @")
  inspect(tokens, content="[Comment(\" @\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__a_4953" {
  let (tokens, _) = @html.tokenize("<!-- A")
  inspect(tokens, content="[Comment(\" A\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__b_4954" {
  let (tokens, _) = @html.tokenize("<!-- B")
  inspect(tokens, content="[Comment(\" B\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__y_4955" {
  let (tokens, _) = @html.tokenize("<!-- Y")
  inspect(tokens, content="[Comment(\" Y\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__z_4956" {
  let (tokens, _) = @html.tokenize("<!-- Z")
  inspect(tokens, content="[Comment(\" Z\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4957" {
  let (tokens, _) = @html.tokenize("<!-- `")
  inspect(tokens, content="[Comment(\" `\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__a_4958" {
  let (tokens, _) = @html.tokenize("<!-- a")
  inspect(tokens, content="[Comment(\" a\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__b_4959" {
  let (tokens, _) = @html.tokenize("<!-- b")
  inspect(tokens, content="[Comment(\" b\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__y_4960" {
  let (tokens, _) = @html.tokenize("<!-- y")
  inspect(tokens, content="[Comment(\" y\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__z_4961" {
  let (tokens, _) = @html.tokenize("<!-- z")
  inspect(tokens, content="[Comment(\" z\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4962" {
  let (tokens, _) = @html.tokenize("<!-- {")
  inspect(tokens, content="[Comment(\" {\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__udbc0udc00_4963" {
  let (tokens, _) = @html.tokenize("<!-- ÙÄÄÄ")
  inspect(tokens, content="[Comment(\" ÙÄÄÄ\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__4964" {
  let (tokens, _) = @html.tokenize("<!--!")
  inspect(tokens, content="[Comment(\"!\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__4965" {
  let (tokens, _) = @html.tokenize("<!--\"")
  inspect(tokens, content="[Comment(\"\\\"\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__4966" {
  let (tokens, _) = @html.tokenize("<!--&")
  inspect(tokens, content="[Comment(\"&\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__4967" {
  let (tokens, _) = @html.tokenize("<!--'")
  inspect(tokens, content="[Comment(\"'\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__4968" {
  let (tokens, _) = @html.tokenize("<!--,")
  inspect(tokens, content="[Comment(\",\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__4969" {
  let (tokens, _) = @html.tokenize("<!---")
  inspect(tokens, content="[Comment(\"\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_u0000_4970" {
  let (tokens, _) = @html.tokenize("<!---\u{0}")
  inspect(tokens, content="[Comment(\"-ÔøΩ\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_u0009_4971" {
  let (tokens, _) = @html.tokenize("<!---\t")
  inspect(tokens, content="[Comment(\"-\\t\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_u000a_4972" {
  let (tokens, _) = @html.tokenize("<!---\n")
  inspect(tokens, content="[Comment(\"-\\n\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_u000b_4973" {
  let (tokens, _) = @html.tokenize("<!---\u{b}")
  inspect(tokens, content="[Comment(\"-\\u{b}\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_u000c_4974" {
  let (tokens, _) = @html.tokenize("<!---\u{c}")
  inspect(tokens, content="[Comment(\"-\\u{c}\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4975" {
  let (tokens, _) = @html.tokenize("<!--- ")
  inspect(tokens, content="[Comment(\"- \"), EOF]")
}

///|
test "html5lib/tokenizer/test3__4976" {
  let (tokens, _) = @html.tokenize("<!---!")
  inspect(tokens, content="[Comment(\"-!\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__4977" {
  let (tokens, _) = @html.tokenize("<!---\"")
  inspect(tokens, content="[Comment(\"-\\\"\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__4978" {
  let (tokens, _) = @html.tokenize("<!---&")
  inspect(tokens, content="[Comment(\"-&\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__4979" {
  let (tokens, _) = @html.tokenize("<!---'")
  inspect(tokens, content="[Comment(\"-'\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__4980" {
  let (tokens, _) = @html.tokenize("<!---,")
  inspect(tokens, content="[Comment(\"-,\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__4981" {
  let (tokens, _) = @html.tokenize("<!----")
  inspect(tokens, content="[Comment(\"\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_u0000_4982" {
  let (tokens, _) = @html.tokenize("<!----\u{0}")
  inspect(tokens, content="[Comment(\"--ÔøΩ\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_u0009_4983" {
  let (tokens, _) = @html.tokenize("<!----\t")
  inspect(tokens, content="[Comment(\"--\\t\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_u000a_4984" {
  let (tokens, _) = @html.tokenize("<!----\n")
  inspect(tokens, content="[Comment(\"--\\n\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_u000b_4985" {
  let (tokens, _) = @html.tokenize("<!----\u{b}")
  inspect(tokens, content="[Comment(\"--\\u{b}\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_u000c_4986" {
  let (tokens, _) = @html.tokenize("<!----\u{c}")
  inspect(tokens, content="[Comment(\"--\\u{c}\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4987" {
  let (tokens, _) = @html.tokenize("<!---- ")
  inspect(tokens, content="[Comment(\"-- \"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4988" {
  let (tokens, _) = @html.tokenize("<!---- -")
  inspect(tokens, content="[Comment(\"-- \"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4989" {
  let (tokens, _) = @html.tokenize("<!---- --")
  inspect(tokens, content="[Comment(\"-- \"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4990" {
  let (tokens, _) = @html.tokenize("<!---- -->")
  inspect(tokens, content="[Comment(\"-- \"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4991" {
  let (tokens, _) = @html.tokenize("<!----  -->")
  inspect(tokens, content="[Comment(\"--  \"), EOF]")
}

///|
test "html5lib/tokenizer/test3__a_4992" {
  let (tokens, _) = @html.tokenize("<!---- a-->")
  inspect(tokens, content="[Comment(\"-- a\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__4993" {
  let (tokens, _) = @html.tokenize("<!----!")
  inspect(tokens, content="[Comment(\"\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__4994" {
  let (tokens, _) = @html.tokenize("<!----!>")
  inspect(tokens, content="[Comment(\"\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___4995" {
  let (tokens, _) = @html.tokenize("<!----! >")
  inspect(tokens, content="[Comment(\"--! >\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_lf_4996" {
  let (tokens, _) = @html.tokenize("<!----!\n>")
  inspect(tokens, content="[Comment(\"--!\\n>\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_cr_4997" {
  let (tokens, _) = @html.tokenize("<!----!\r>")
  inspect(tokens, content="[Comment(\"--!\\n>\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_crlf_4998" {
  let (tokens, _) = @html.tokenize("<!----!\r\n>")
  inspect(tokens, content="[Comment(\"--!\\n>\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_a_4999" {
  let (tokens, _) = @html.tokenize("<!----!a")
  inspect(tokens, content="[Comment(\"--!a\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_a_5000" {
  let (tokens, _) = @html.tokenize("<!----!a-")
  inspect(tokens, content="[Comment(\"--!a\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_a_5001" {
  let (tokens, _) = @html.tokenize("<!----!a--")
  inspect(tokens, content="[Comment(\"--!a\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_a_5002" {
  let (tokens, _) = @html.tokenize("<!----!a-->")
  inspect(tokens, content="[Comment(\"--!a\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5003" {
  let (tokens, _) = @html.tokenize("<!----!-")
  inspect(tokens, content="[Comment(\"--!\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5004" {
  let (tokens, _) = @html.tokenize("<!----!--")
  inspect(tokens, content="[Comment(\"--!\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5005" {
  let (tokens, _) = @html.tokenize("<!----!-->")
  inspect(tokens, content="[Comment(\"--!\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5006" {
  let (tokens, _) = @html.tokenize("<!----\"")
  inspect(tokens, content="[Comment(\"--\\\"\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5007" {
  let (tokens, _) = @html.tokenize("<!----&")
  inspect(tokens, content="[Comment(\"--&\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5008" {
  let (tokens, _) = @html.tokenize("<!----'")
  inspect(tokens, content="[Comment(\"--'\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5009" {
  let (tokens, _) = @html.tokenize("<!----,")
  inspect(tokens, content="[Comment(\"--,\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5010" {
  let (tokens, _) = @html.tokenize("<!-----")
  inspect(tokens, content="[Comment(\"-\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5011" {
  let (tokens, _) = @html.tokenize("<!----.")
  inspect(tokens, content="[Comment(\"--.\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5012" {
  let (tokens, _) = @html.tokenize("<!----/")
  inspect(tokens, content="[Comment(\"--/\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_0_5013" {
  let (tokens, _) = @html.tokenize("<!----0")
  inspect(tokens, content="[Comment(\"--0\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_1_5014" {
  let (tokens, _) = @html.tokenize("<!----1")
  inspect(tokens, content="[Comment(\"--1\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_9_5015" {
  let (tokens, _) = @html.tokenize("<!----9")
  inspect(tokens, content="[Comment(\"--9\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5016" {
  let (tokens, _) = @html.tokenize("<!----<")
  inspect(tokens, content="[Comment(\"--<\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5017" {
  let (tokens, _) = @html.tokenize("<!----=")
  inspect(tokens, content="[Comment(\"--=\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5018" {
  let (tokens, _) = @html.tokenize("<!---->")
  inspect(tokens, content="[Comment(\"\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5019" {
  let (tokens, _) = @html.tokenize("<!----?")
  inspect(tokens, content="[Comment(\"--?\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5020" {
  let (tokens, _) = @html.tokenize("<!----@")
  inspect(tokens, content="[Comment(\"--@\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_a_5021" {
  let (tokens, _) = @html.tokenize("<!----A")
  inspect(tokens, content="[Comment(\"--A\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_b_5022" {
  let (tokens, _) = @html.tokenize("<!----B")
  inspect(tokens, content="[Comment(\"--B\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_y_5023" {
  let (tokens, _) = @html.tokenize("<!----Y")
  inspect(tokens, content="[Comment(\"--Y\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_z_5024" {
  let (tokens, _) = @html.tokenize("<!----Z")
  inspect(tokens, content="[Comment(\"--Z\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5025" {
  let (tokens, _) = @html.tokenize("<!----`")
  inspect(tokens, content="[Comment(\"--`\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_a_5026" {
  let (tokens, _) = @html.tokenize("<!----a")
  inspect(tokens, content="[Comment(\"--a\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_b_5027" {
  let (tokens, _) = @html.tokenize("<!----b")
  inspect(tokens, content="[Comment(\"--b\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_y_5028" {
  let (tokens, _) = @html.tokenize("<!----y")
  inspect(tokens, content="[Comment(\"--y\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_z_5029" {
  let (tokens, _) = @html.tokenize("<!----z")
  inspect(tokens, content="[Comment(\"--z\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5030" {
  let (tokens, _) = @html.tokenize("<!----{")
  inspect(tokens, content="[Comment(\"--{\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_udbc0udc00_5031" {
  let (tokens, _) = @html.tokenize("<!----ÙÄÄÄ")
  inspect(tokens, content="[Comment(\"--ÙÄÄÄ\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5032" {
  let (tokens, _) = @html.tokenize("<!---.")
  inspect(tokens, content="[Comment(\"-.\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5033" {
  let (tokens, _) = @html.tokenize("<!---/")
  inspect(tokens, content="[Comment(\"-/\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_0_5034" {
  let (tokens, _) = @html.tokenize("<!---0")
  inspect(tokens, content="[Comment(\"-0\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_1_5035" {
  let (tokens, _) = @html.tokenize("<!---1")
  inspect(tokens, content="[Comment(\"-1\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_9_5036" {
  let (tokens, _) = @html.tokenize("<!---9")
  inspect(tokens, content="[Comment(\"-9\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5037" {
  let (tokens, _) = @html.tokenize("<!---<")
  inspect(tokens, content="[Comment(\"-<\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5038" {
  let (tokens, _) = @html.tokenize("<!---=")
  inspect(tokens, content="[Comment(\"-=\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5039" {
  let (tokens, _) = @html.tokenize("<!---?")
  inspect(tokens, content="[Comment(\"-?\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5040" {
  let (tokens, _) = @html.tokenize("<!---@")
  inspect(tokens, content="[Comment(\"-@\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_a_5041" {
  let (tokens, _) = @html.tokenize("<!---A")
  inspect(tokens, content="[Comment(\"-A\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_b_5042" {
  let (tokens, _) = @html.tokenize("<!---B")
  inspect(tokens, content="[Comment(\"-B\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_y_5043" {
  let (tokens, _) = @html.tokenize("<!---Y")
  inspect(tokens, content="[Comment(\"-Y\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_z_5044" {
  let (tokens, _) = @html.tokenize("<!---Z")
  inspect(tokens, content="[Comment(\"-Z\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5045" {
  let (tokens, _) = @html.tokenize("<!---`")
  inspect(tokens, content="[Comment(\"-`\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_a_5046" {
  let (tokens, _) = @html.tokenize("<!---a")
  inspect(tokens, content="[Comment(\"-a\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_b_5047" {
  let (tokens, _) = @html.tokenize("<!---b")
  inspect(tokens, content="[Comment(\"-b\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_y_5048" {
  let (tokens, _) = @html.tokenize("<!---y")
  inspect(tokens, content="[Comment(\"-y\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_z_5049" {
  let (tokens, _) = @html.tokenize("<!---z")
  inspect(tokens, content="[Comment(\"-z\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5050" {
  let (tokens, _) = @html.tokenize("<!---{")
  inspect(tokens, content="[Comment(\"-{\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_udbc0udc00_5051" {
  let (tokens, _) = @html.tokenize("<!---ÙÄÄÄ")
  inspect(tokens, content="[Comment(\"-ÙÄÄÄ\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5052" {
  let (tokens, _) = @html.tokenize("<!--.")
  inspect(tokens, content="[Comment(\".\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5053" {
  let (tokens, _) = @html.tokenize("<!--/")
  inspect(tokens, content="[Comment(\"/\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_0_5054" {
  let (tokens, _) = @html.tokenize("<!--0")
  inspect(tokens, content="[Comment(\"0\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_1_5055" {
  let (tokens, _) = @html.tokenize("<!--1")
  inspect(tokens, content="[Comment(\"1\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_9_5056" {
  let (tokens, _) = @html.tokenize("<!--9")
  inspect(tokens, content="[Comment(\"9\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5057" {
  let (tokens, _) = @html.tokenize("<!--<")
  inspect(tokens, content="[Comment(\"<\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5058" {
  let (tokens, _) = @html.tokenize("<!--=")
  inspect(tokens, content="[Comment(\"=\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5059" {
  let (tokens, _) = @html.tokenize("<!--?")
  inspect(tokens, content="[Comment(\"?\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5060" {
  let (tokens, _) = @html.tokenize("<!--@")
  inspect(tokens, content="[Comment(\"@\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_a_5061" {
  let (tokens, _) = @html.tokenize("<!--A")
  inspect(tokens, content="[Comment(\"A\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_b_5062" {
  let (tokens, _) = @html.tokenize("<!--B")
  inspect(tokens, content="[Comment(\"B\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_y_5063" {
  let (tokens, _) = @html.tokenize("<!--Y")
  inspect(tokens, content="[Comment(\"Y\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_z_5064" {
  let (tokens, _) = @html.tokenize("<!--Z")
  inspect(tokens, content="[Comment(\"Z\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5065" {
  let (tokens, _) = @html.tokenize("<!--`")
  inspect(tokens, content="[Comment(\"`\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_a_5066" {
  let (tokens, _) = @html.tokenize("<!--a")
  inspect(tokens, content="[Comment(\"a\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_b_5067" {
  let (tokens, _) = @html.tokenize("<!--b")
  inspect(tokens, content="[Comment(\"b\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_y_5068" {
  let (tokens, _) = @html.tokenize("<!--y")
  inspect(tokens, content="[Comment(\"y\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_z_5069" {
  let (tokens, _) = @html.tokenize("<!--z")
  inspect(tokens, content="[Comment(\"z\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5070" {
  let (tokens, _) = @html.tokenize("<!--{")
  inspect(tokens, content="[Comment(\"{\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_udbc0udc00_5071" {
  let (tokens, _) = @html.tokenize("<!--ÙÄÄÄ")
  inspect(tokens, content="[Comment(\"ÙÄÄÄ\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5072" {
  let (tokens, _) = @html.tokenize("<!/")
  inspect(tokens, content="[Comment(\"/\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_0_5073" {
  let (tokens, _) = @html.tokenize("<!0")
  inspect(tokens, content="[Comment(\"0\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_1_5074" {
  let (tokens, _) = @html.tokenize("<!1")
  inspect(tokens, content="[Comment(\"1\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_9_5075" {
  let (tokens, _) = @html.tokenize("<!9")
  inspect(tokens, content="[Comment(\"9\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5076" {
  let (tokens, _) = @html.tokenize("<!<")
  inspect(tokens, content="[Comment(\"<\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5077" {
  let (tokens, _) = @html.tokenize("<!=")
  inspect(tokens, content="[Comment(\"=\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5078" {
  let (tokens, _) = @html.tokenize("<!>")
  inspect(tokens, content="[Comment(\"\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5079" {
  let (tokens, _) = @html.tokenize("<!?")
  inspect(tokens, content="[Comment(\"?\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5080" {
  let (tokens, _) = @html.tokenize("<!@")
  inspect(tokens, content="[Comment(\"@\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_a_5081" {
  let (tokens, _) = @html.tokenize("<!A")
  inspect(tokens, content="[Comment(\"A\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_b_5082" {
  let (tokens, _) = @html.tokenize("<!B")
  inspect(tokens, content="[Comment(\"B\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_doctype_5083" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE")
  inspect(
    tokens,
    content="[DOCTYPE(name=None, public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypeu0000_5084" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE\u{0}")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"ÔøΩ\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypeu0008_5085" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE\u{8}")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"\\u{8}\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypeu0009_5086" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE\t")
  inspect(
    tokens,
    content="[DOCTYPE(name=None, public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypeu000a_5087" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE\n")
  inspect(
    tokens,
    content="[DOCTYPE(name=None, public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypeu000b_5088" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE\u{b}")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"\\u{b}\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypeu000c_5089" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE\u{c}")
  inspect(
    tokens,
    content="[DOCTYPE(name=None, public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypeu000d_5090" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE\r")
  inspect(
    tokens,
    content="[DOCTYPE(name=None, public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypeu001f_5091" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE\u{1f}")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"\\u{1f}\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype__5092" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE ")
  inspect(
    tokens,
    content="[DOCTYPE(name=None, public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype_u0000_5093" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE \u{0}")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"ÔøΩ\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype_u0008_5094" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE \u{8}")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"\\u{8}\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype_u0009_5095" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE \t")
  inspect(
    tokens,
    content="[DOCTYPE(name=None, public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype_u000a_5096" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE \n")
  inspect(
    tokens,
    content="[DOCTYPE(name=None, public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype_u000b_5097" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE \u{b}")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"\\u{b}\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype_u000c_5098" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE \u{c}")
  inspect(
    tokens,
    content="[DOCTYPE(name=None, public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype_u000d_5099" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE \r")
  inspect(
    tokens,
    content="[DOCTYPE(name=None, public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype_u001f_5100" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE \u{1f}")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"\\u{1f}\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype__5101" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE  ")
  inspect(
    tokens,
    content="[DOCTYPE(name=None, public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype__5102" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE !")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"!\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype__5103" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE \"")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"\\\"\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype__5104" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE &")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"&\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype__5105" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE '")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"'\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype__5106" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE -")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"-\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype__5107" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE /")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"/\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype_0_5108" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE 0")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"0\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype_1_5109" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE 1")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"1\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype_9_5110" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE 9")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"9\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype__5111" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE <")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"<\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype__5112" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE =")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"=\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype__5113" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE >")
  inspect(
    tokens,
    content="[DOCTYPE(name=None, public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype__5114" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE ?")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"?\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype__5115" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE @")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"@\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype_a_5116" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE A")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype_b_5117" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE B")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"b\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype_y_5118" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE Y")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"y\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype_z_5119" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE Z")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"z\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype__5120" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE [")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"[\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype__5121" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE `")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"`\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype_a_5122" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE a")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype_au0000_5123" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE a\u{0}")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"aÔøΩ\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype_au0008_5124" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE a\u{8}")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\\u{8}\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype_au0009_5125" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE a\t")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype_au000a_5126" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE a\n")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype_au000b_5127" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE a\u{b}")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\\u{b}\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype_au000c_5128" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE a\u{c}")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype_au000d_5129" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE a\r")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype_au001f_5130" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE a\u{1f}")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\\u{1f}\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype_a__5131" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE a ")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype_a_u0000_5132" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE a \u{0}")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype_a_u0008_5133" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE a \u{8}")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype_a_u0009_5134" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE a \t")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype_a_u000a_5135" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE a \n")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

// ============================================================================
// AUTO-GENERATED FILE - DO NOT MODIFY MANUALLY
// Generated by: scripts/generate_conformance_tests.py
// Regenerate with: python3 scripts/generate_conformance_tests.py
// ============================================================================

// Copyright 2025 International Digital Economy Academy
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

///|
/// html5lib Tokenizer Conformance Tests (continued)
/// Source: https://github.com/html5lib/html5lib-tests

///|
test "html5lib/tokenizer/test3_doctypea_public_5636" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC''-")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public_5637" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC''/")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public0_5638" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC''0")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public1_5639" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC''1")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public9_5640" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC''9")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public_5641" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC''<")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public_5642" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC''=")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public_5643" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC''>")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"\"), system_id=None, force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public_5644" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC''?")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public_5645" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC''@")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_publica_5646" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC''A")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_publicb_5647" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC''B")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_publicy_5648" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC''Y")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_publicz_5649" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC''Z")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public_5650" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC''`")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_publica_5651" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC''a")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_publicb_5652" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC''b")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_publicy_5653" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC''y")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_publicz_5654" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC''z")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public_5655" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC''{")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_publicudbc0udc00_5656" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC''􀀀")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public_5657" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC'(")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"(\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public_5658" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC'-")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"-\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public_5659" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC'/")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"/\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public0_5660" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC'0")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"0\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public1_5661" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC'1")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"1\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public9_5662" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC'9")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"9\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public_5663" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC'<")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"<\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public_5664" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC'=")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"=\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public_5665" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC'>")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public_5666" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC'?")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"?\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public_5667" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC'@")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"@\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_publica_5668" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC'A")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"A\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_publicb_5669" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC'B")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"B\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_publicy_5670" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC'Y")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"Y\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_publicz_5671" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC'Z")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"Z\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public_5672" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC'`")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"`\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_publica_5673" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC'a")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"a\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_publicb_5674" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC'b")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"b\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_publicy_5675" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC'y")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"y\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_publicz_5676" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC'z")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"z\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public_5677" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC'{")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"{\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_publicudbc0udc00_5678" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC'􀀀")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=Some(\"􀀀\"), system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public_5679" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC(")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public_5680" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC-")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public_5681" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC/")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public0_5682" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC0")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public1_5683" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC1")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public9_5684" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC9")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public_5685" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC<")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public_5686" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC=")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public_5687" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC>")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public_5688" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC?")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public_5689" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC@")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_publica_5690" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLICA")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_publicb_5691" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLICB")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_publicy_5692" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLICY")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_publicz_5693" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLICZ")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public_5694" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC`")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_publica_5695" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLICa")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_publicb_5696" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLICb")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_publicy_5697" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLICy")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_publicz_5698" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLICz")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_public_5699" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC{")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_publicudbc0udc00_5700" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa PUBLIC􀀀")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5701" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemu0000_5702" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\u{00}")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemu0008_5703" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\b")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemu0009_5704" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\t")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemu000a_5705" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\n")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemu000b_5706" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\u{0b}")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemu000c_5707" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\u{0c}")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemu000d_5708" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\r")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemu001f_5709" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\u{1f}")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system__5710" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM ")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5711" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM!")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5712" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\"")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemu0000_5713" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\"\u{00}")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"�\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemu0009_5714" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\"\t")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\\t\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemu000a_5715" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\"\n")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\\n\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemu000b_5716" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\"\u{0b}")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\\u{0b}\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemu000c_5717" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\"\u{0c}")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\\u{0c}\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system__5718" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\" ")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\" \"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5719" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\"!")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"!\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5720" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\"\"")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5721" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\"#")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"#\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5722" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\"&")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"&\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5723" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\"'")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"'\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5724" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\"-")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"-\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5725" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\"/")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"/\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system0_5726" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\"0")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"0\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system1_5727" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\"1")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"1\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system9_5728" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\"9")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"9\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5729" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\"<")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"<\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5730" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\"=")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"=\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5731" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\">")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5732" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\"?")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"?\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5733" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\"@")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"@\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systema_5734" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\"A")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"A\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemb_5735" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\"B")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"B\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemy_5736" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\"Y")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"Y\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemz_5737" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\"Z")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"Z\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5738" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\"`")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"`\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systema_5739" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\"a")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"a\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemb_5740" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\"b")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"b\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemy_5741" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\"y")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"y\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemz_5742" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\"z")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"z\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5743" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\"{")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"{\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemudbc0udc00_5744" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM\"􀀀")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"􀀀\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5745" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM#")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5746" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM&")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5747" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemu0000_5748" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'\u{00}")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"�\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemu0009_5749" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'\t")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\\t\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemu000a_5750" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'\n")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\\n\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemu000b_5751" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'\u{0b}")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\\u{0b}\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemu000c_5752" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'\u{0c}")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\\u{0c}\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system__5753" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM' ")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\" \"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5754" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'!")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"!\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5755" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'\"")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\\\"\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5756" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'&")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"&\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5757" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemu0000_5758" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''\u{00}")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemu0008_5759" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''\b")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemu0009_5760" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''\t")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemu000a_5761" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''\n")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemu000b_5762" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''\u{0b}")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemu000c_5763" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''\u{0c}")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemu000d_5764" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''\r")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemu001f_5765" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''\u{1f}")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system__5766" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'' ")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5767" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''!")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5768" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''\"")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5769" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''&")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5770" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'''")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5771" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''-")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5772" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''/")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system0_5773" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''0")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system1_5774" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''1")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system9_5775" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''9")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5776" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''<")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5777" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''=")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5778" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''>")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5779" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''?")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5780" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''@")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systema_5781" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''A")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemb_5782" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''B")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemy_5783" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''Y")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemz_5784" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''Z")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5785" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''`")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systema_5786" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''a")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemb_5787" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''b")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemy_5788" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''y")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemz_5789" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''z")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5790" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''{")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemudbc0udc00_5791" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM''􀀀")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5792" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'(")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"(\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5793" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'-")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"-\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5794" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'/")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"/\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system0_5795" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'0")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"0\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system1_5796" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'1")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"1\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system9_5797" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'9")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"9\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5798" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'<")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"<\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5799" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'=")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"=\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5800" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'>")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5801" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'?")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"?\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5802" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'@")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"@\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systema_5803" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'A")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"A\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemb_5804" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'B")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"B\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemy_5805" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'Y")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"Y\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemz_5806" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'Z")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"Z\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5807" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'`")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"`\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systema_5808" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'a")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"a\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemb_5809" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'b")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"b\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemy_5810" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'y")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"y\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemz_5811" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'z")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"z\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5812" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'{")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"{\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemudbc0udc00_5813" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM'􀀀")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=Some(\"􀀀\"), force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5814" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM(")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5815" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM-")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5816" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM/")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system0_5817" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM0")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system1_5818" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM1")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system9_5819" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM9")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5820" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM<")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5821" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM=")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5822" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM>")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5823" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM?")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5824" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM@")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systema_5825" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEMA")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemb_5826" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEMB")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemy_5827" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEMY")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemz_5828" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEMZ")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5829" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM`")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systema_5830" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEMa")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemb_5831" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEMb")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemy_5832" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEMy")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemz_5833" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEMz")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_system_5834" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM{")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_systemudbc0udc00_5835" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa SYSTEM􀀀")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_y_5836" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa Y")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_z_5837" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa Z")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea__5838" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa `")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_a_5839" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa a")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_au0000_5840" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa a\u{00}")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_au0009_5841" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa a\t")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_au000a_5842" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa a\n")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_au000b_5843" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa a\u{0b}")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_au000c_5844" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa a\u{0c}")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_a__5845" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa a ")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_a_5846" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa a!")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_a_5847" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa a\"")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_a_5848" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa a&")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_a_5849" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa a'")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_a_5850" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa a-")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_a_5851" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa a/")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_a0_5852" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa a0")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_a1_5853" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa a1")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_a9_5854" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa a9")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_a_5855" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa a<")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_a_5856" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa a=")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_a_5857" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa a>")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_a_5858" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa a?")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_a_5859" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa a@")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_aa_5860" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa aA")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_ab_5861" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa aB")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_ay_5862" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa aY")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_az_5863" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa aZ")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_a_5864" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa a`")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_aa_5865" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa aa")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_ab_5866" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa ab")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_ay_5867" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa ay")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_az_5868" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa az")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_a_5869" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa a{")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_audbc0udc00_5870" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa a􀀀")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_b_5871" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa b")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_y_5872" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa y")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_z_5873" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa z")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea__5874" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa {")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_udbc0udc00_5875" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa 􀀀")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_5876" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa!")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a!\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_5877" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa\"")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\\\"\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_5878" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa&")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a&\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_5879" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa'")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a'\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_5880" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa-")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a-\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_5881" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa/")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a/\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea0_5882" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa0")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a0\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea1_5883" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa1")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a1\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea9_5884" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa9")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a9\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_5885" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa<")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a<\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_5886" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa=")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a=\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_5887" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa>")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a\"), public_id=None, system_id=None, force_quirks=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_5888" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa?")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a?\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_5889" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa@")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a@\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypeaa_5890" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEaA")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"aa\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypeab_5891" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEaB")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"ab\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypeay_5892" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEaY")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"ay\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypeaz_5893" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEaZ")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"az\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_5894" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa[")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a[\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_5895" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa`")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a`\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypeaa_5896" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEaa")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"aa\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypeab_5897" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEab")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"ab\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypeay_5898" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEay")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"ay\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypeaz_5899" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEaz")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"az\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypea_5900" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa{")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a{\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypeaudbc0udc00_5901" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEa􀀀")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"a􀀀\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypeb_5902" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEb")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"b\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypey_5903" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEy")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"y\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypez_5904" {
  let (tokens, _) = @html.tokenize("<!DOCTYPEz")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"z\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctype_5905" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE{")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"{\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_doctypeudbc0udc00_5906" {
  let (tokens, _) = @html.tokenize("<!DOCTYPE􀀀")
  inspect(
    tokens,
    content="[DOCTYPE(name=Some(\"􀀀\"), public_id=None, system_id=None, force_quirks=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_y_5907" {
  let (tokens, _) = @html.tokenize("<!Y")
  inspect(tokens, content="[Comment(\"Y\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_z_5908" {
  let (tokens, _) = @html.tokenize("<!Z")
  inspect(tokens, content="[Comment(\"Z\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5909" {
  let (tokens, _) = @html.tokenize("<!`")
  inspect(tokens, content="[Comment(\"`\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_a_5910" {
  let (tokens, _) = @html.tokenize("<!a")
  inspect(tokens, content="[Comment(\"a\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_b_5911" {
  let (tokens, _) = @html.tokenize("<!b")
  inspect(tokens, content="[Comment(\"b\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_y_5912" {
  let (tokens, _) = @html.tokenize("<!y")
  inspect(tokens, content="[Comment(\"y\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_z_5913" {
  let (tokens, _) = @html.tokenize("<!z")
  inspect(tokens, content="[Comment(\"z\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5914" {
  let (tokens, _) = @html.tokenize("<!{")
  inspect(tokens, content="[Comment(\"{\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_udbc0udc00_5915" {
  let (tokens, _) = @html.tokenize("<!􀀀")
  inspect(tokens, content="[Comment(\"􀀀\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5916" {
  let (tokens, _) = @html.tokenize("<\"")
  inspect(tokens, content="[Character('<'), Character('\"'), EOF]")
}

///|
test "html5lib/tokenizer/test3__5917" {
  let (tokens, _) = @html.tokenize("<&")
  inspect(tokens, content="[Character('<'), Character('&'), EOF]")
}

///|
test "html5lib/tokenizer/test3__5918" {
  let (tokens, _) = @html.tokenize("<'")
  inspect(tokens, content="[Character('<'), Character('\\''), EOF]")
}

///|
test "html5lib/tokenizer/test3__5919" {
  let (tokens, _) = @html.tokenize("<-")
  inspect(tokens, content="[Character('<'), Character('-'), EOF]")
}

///|
test "html5lib/tokenizer/test3__5920" {
  let (tokens, _) = @html.tokenize("<.")
  inspect(tokens, content="[Character('<'), Character('.'), EOF]")
}

///|
test "html5lib/tokenizer/test3__5921" {
  let (tokens, _) = @html.tokenize("</")
  inspect(tokens, content="[Character('<'), Character('/'), EOF]")
}

///|
test "html5lib/tokenizer/test3_u0000_5922" {
  let (tokens, _) = @html.tokenize("</\u{00}")
  inspect(tokens, content="[Comment(\"�\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_u0009_5923" {
  let (tokens, _) = @html.tokenize("</\t")
  inspect(tokens, content="[Comment(\"\\t\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_u000a_5924" {
  let (tokens, _) = @html.tokenize("</\n")
  inspect(tokens, content="[Comment(\"\\n\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_u000b_5925" {
  let (tokens, _) = @html.tokenize("</\u{0b}")
  inspect(tokens, content="[Comment(\"\\u{0b}\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_u000c_5926" {
  let (tokens, _) = @html.tokenize("</\u{0c}")
  inspect(tokens, content="[Comment(\"\\u{0c}\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___5927" {
  let (tokens, _) = @html.tokenize("</ ")
  inspect(tokens, content="[Comment(\" \"), EOF]")
}

///|
test "html5lib/tokenizer/test3__u0000_5928" {
  let (tokens, _) = @html.tokenize("</ \u{00}")
  inspect(tokens, content="[Comment(\" �\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5929" {
  let (tokens, _) = @html.tokenize("</!")
  inspect(tokens, content="[Comment(\"!\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5930" {
  let (tokens, _) = @html.tokenize("</\"")
  inspect(tokens, content="[Comment(\"\\\"\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5931" {
  let (tokens, _) = @html.tokenize("</&")
  inspect(tokens, content="[Comment(\"&\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5932" {
  let (tokens, _) = @html.tokenize("</'")
  inspect(tokens, content="[Comment(\"'\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5933" {
  let (tokens, _) = @html.tokenize("</-")
  inspect(tokens, content="[Comment(\"-\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5934" {
  let (tokens, _) = @html.tokenize("<//")
  inspect(tokens, content="[Comment(\"/\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_0_5935" {
  let (tokens, _) = @html.tokenize("</0")
  inspect(tokens, content="[Comment(\"0\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_1_5936" {
  let (tokens, _) = @html.tokenize("</1")
  inspect(tokens, content="[Comment(\"1\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_9_5937" {
  let (tokens, _) = @html.tokenize("</9")
  inspect(tokens, content="[Comment(\"9\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5938" {
  let (tokens, _) = @html.tokenize("</<")
  inspect(tokens, content="[Comment(\"<\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5939" {
  let (tokens, _) = @html.tokenize("</=")
  inspect(tokens, content="[Comment(\"=\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5940" {
  let (tokens, _) = @html.tokenize("</>")
  inspect(tokens, content="[EOF]")
}

///|
test "html5lib/tokenizer/test3__5941" {
  let (tokens, _) = @html.tokenize("</?")
  inspect(tokens, content="[Comment(\"?\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5942" {
  let (tokens, _) = @html.tokenize("</@")
  inspect(tokens, content="[Comment(\"@\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_a_5943" {
  let (tokens, _) = @html.tokenize("</A>")
  inspect(tokens, content="[EndTag(name=\"a\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_b_5944" {
  let (tokens, _) = @html.tokenize("</B>")
  inspect(tokens, content="[EndTag(name=\"b\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_y_5945" {
  let (tokens, _) = @html.tokenize("</Y>")
  inspect(tokens, content="[EndTag(name=\"y\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_z_5946" {
  let (tokens, _) = @html.tokenize("</Z>")
  inspect(tokens, content="[EndTag(name=\"z\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5947" {
  let (tokens, _) = @html.tokenize("</[")
  inspect(tokens, content="[Comment(\"[\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5948" {
  let (tokens, _) = @html.tokenize("</`")
  inspect(tokens, content="[Comment(\"`\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_a_5949" {
  let (tokens, _) = @html.tokenize("</a>")
  inspect(tokens, content="[EndTag(name=\"a\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_b_5950" {
  let (tokens, _) = @html.tokenize("</b>")
  inspect(tokens, content="[EndTag(name=\"b\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_y_5951" {
  let (tokens, _) = @html.tokenize("</y>")
  inspect(tokens, content="[EndTag(name=\"y\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_z_5952" {
  let (tokens, _) = @html.tokenize("</z>")
  inspect(tokens, content="[EndTag(name=\"z\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5953" {
  let (tokens, _) = @html.tokenize("</{")
  inspect(tokens, content="[Comment(\"{\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_udbc0udc00_5954" {
  let (tokens, _) = @html.tokenize("</􀀀")
  inspect(tokens, content="[Comment(\"􀀀\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_0_5955" {
  let (tokens, _) = @html.tokenize("<0")
  inspect(tokens, content="[Character('<'), Character('0'), EOF]")
}

///|
test "html5lib/tokenizer/test3_1_5956" {
  let (tokens, _) = @html.tokenize("<1")
  inspect(tokens, content="[Character('<'), Character('1'), EOF]")
}

///|
test "html5lib/tokenizer/test3_9_5957" {
  let (tokens, _) = @html.tokenize("<9")
  inspect(tokens, content="[Character('<'), Character('9'), EOF]")
}

///|
test "html5lib/tokenizer/test3__5958" {
  let (tokens, _) = @html.tokenize("<<")
  inspect(tokens, content="[Character('<'), Character('<'), EOF]")
}

///|
test "html5lib/tokenizer/test3__5959" {
  let (tokens, _) = @html.tokenize("<=")
  inspect(tokens, content="[Character('<'), Character('='), EOF]")
}

///|
test "html5lib/tokenizer/test3__5960" {
  let (tokens, _) = @html.tokenize("<>")
  inspect(tokens, content="[Character('<'), Character('>'), EOF]")
}

///|
test "html5lib/tokenizer/test3__5961" {
  let (tokens, _) = @html.tokenize("<?")
  inspect(tokens, content="[Comment(\"?\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_u0000_5962" {
  let (tokens, _) = @html.tokenize("<?\u{00}")
  inspect(tokens, content="[Comment(\"?�\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_u0009_5963" {
  let (tokens, _) = @html.tokenize("<?\t")
  inspect(tokens, content="[Comment(\"?\\t\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_u000a_5964" {
  let (tokens, _) = @html.tokenize("<?\n")
  inspect(tokens, content="[Comment(\"?\\n\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_u000b_5965" {
  let (tokens, _) = @html.tokenize("<?\u{0b}")
  inspect(tokens, content="[Comment(\"?\\u{0b}\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_u000c_5966" {
  let (tokens, _) = @html.tokenize("<?\u{0c}")
  inspect(tokens, content="[Comment(\"?\\u{0c}\"), EOF]")
}

///|
test "html5lib/tokenizer/test3___5967" {
  let (tokens, _) = @html.tokenize("<? ")
  inspect(tokens, content="[Comment(\"? \"), EOF]")
}

///|
test "html5lib/tokenizer/test3__u0000_5968" {
  let (tokens, _) = @html.tokenize("<? \u{00}")
  inspect(tokens, content="[Comment(\"? �\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5969" {
  let (tokens, _) = @html.tokenize("<?!")
  inspect(tokens, content="[Comment(\"?!\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5970" {
  let (tokens, _) = @html.tokenize("<?\"")
  inspect(tokens, content="[Comment(\"?\\\"\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5971" {
  let (tokens, _) = @html.tokenize("<?&")
  inspect(tokens, content="[Comment(\"?&\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5972" {
  let (tokens, _) = @html.tokenize("<?'")
  inspect(tokens, content="[Comment(\"?'\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5973" {
  let (tokens, _) = @html.tokenize("<?-")
  inspect(tokens, content="[Comment(\"?-\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5974" {
  let (tokens, _) = @html.tokenize("<?/")
  inspect(tokens, content="[Comment(\"?/\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_0_5975" {
  let (tokens, _) = @html.tokenize("<?0")
  inspect(tokens, content="[Comment(\"?0\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_1_5976" {
  let (tokens, _) = @html.tokenize("<?1")
  inspect(tokens, content="[Comment(\"?1\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_9_5977" {
  let (tokens, _) = @html.tokenize("<?9")
  inspect(tokens, content="[Comment(\"?9\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5978" {
  let (tokens, _) = @html.tokenize("<?<")
  inspect(tokens, content="[Comment(\"?<\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5979" {
  let (tokens, _) = @html.tokenize("<?=")
  inspect(tokens, content="[Comment(\"?=\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5980" {
  let (tokens, _) = @html.tokenize("<?>")
  inspect(tokens, content="[Comment(\"?\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5981" {
  let (tokens, _) = @html.tokenize("<??")
  inspect(tokens, content="[Comment(\"??\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5982" {
  let (tokens, _) = @html.tokenize("<?@")
  inspect(tokens, content="[Comment(\"?@\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_a_5983" {
  let (tokens, _) = @html.tokenize("<?A")
  inspect(tokens, content="[Comment(\"?A\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_b_5984" {
  let (tokens, _) = @html.tokenize("<?B")
  inspect(tokens, content="[Comment(\"?B\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_y_5985" {
  let (tokens, _) = @html.tokenize("<?Y")
  inspect(tokens, content="[Comment(\"?Y\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_z_5986" {
  let (tokens, _) = @html.tokenize("<?Z")
  inspect(tokens, content="[Comment(\"?Z\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5987" {
  let (tokens, _) = @html.tokenize("<?`")
  inspect(tokens, content="[Comment(\"?`\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_a_5988" {
  let (tokens, _) = @html.tokenize("<?a")
  inspect(tokens, content="[Comment(\"?a\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_b_5989" {
  let (tokens, _) = @html.tokenize("<?b")
  inspect(tokens, content="[Comment(\"?b\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_y_5990" {
  let (tokens, _) = @html.tokenize("<?y")
  inspect(tokens, content="[Comment(\"?y\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_z_5991" {
  let (tokens, _) = @html.tokenize("<?z")
  inspect(tokens, content="[Comment(\"?z\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5992" {
  let (tokens, _) = @html.tokenize("<?{")
  inspect(tokens, content="[Comment(\"?{\"), EOF]")
}

///|
test "html5lib/tokenizer/test3_udbc0udc00_5993" {
  let (tokens, _) = @html.tokenize("<?􀀀")
  inspect(tokens, content="[Comment(\"?􀀀\"), EOF]")
}

///|
test "html5lib/tokenizer/test3__5994" {
  let (tokens, _) = @html.tokenize("<@")
  inspect(tokens, content="[Character('<'), Character('@'), EOF]")
}

///|
test "html5lib/tokenizer/test3_a_5995" {
  let (tokens, _) = @html.tokenize("<A>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_b_5996" {
  let (tokens, _) = @html.tokenize("<B>")
  inspect(
    tokens,
    content="[StartTag(name=\"b\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_y_5997" {
  let (tokens, _) = @html.tokenize("<Y>")
  inspect(
    tokens,
    content="[StartTag(name=\"y\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_z_5998" {
  let (tokens, _) = @html.tokenize("<Z>")
  inspect(
    tokens,
    content="[StartTag(name=\"z\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3__5999" {
  let (tokens, _) = @html.tokenize("<[")
  inspect(tokens, content="[Character('<'), Character('['), EOF]")
}

///|
test "html5lib/tokenizer/test3__6000" {
  let (tokens, _) = @html.tokenize("<`")
  inspect(tokens, content="[Character('<'), Character('`'), EOF]")
}

///|
test "html5lib/tokenizer/test3_a_6001" {
  let (tokens, _) = @html.tokenize("<a>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_au0000_6002" {
  let (tokens, _) = @html.tokenize("<a\u{00}>")
  inspect(
    tokens,
    content="[StartTag(name=\"a�\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_au0008_6003" {
  let (tokens, _) = @html.tokenize("<a\b>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\\b\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_au0009_6004" {
  let (tokens, _) = @html.tokenize("<a\t>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_au000a_6005" {
  let (tokens, _) = @html.tokenize("<a\n>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_au000b_6006" {
  let (tokens, _) = @html.tokenize("<a\u{0b}>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\\u{0b}\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_au000c_6007" {
  let (tokens, _) = @html.tokenize("<a\u{0c}>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_au000d_6008" {
  let (tokens, _) = @html.tokenize("<a\r>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_au001f_6009" {
  let (tokens, _) = @html.tokenize("<a\u{1f}>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\\u{1f}\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a__6010" {
  let (tokens, _) = @html.tokenize("<a >")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_u0000_6011" {
  let (tokens, _) = @html.tokenize("<a \u{00}>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"�\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_u0008_6012" {
  let (tokens, _) = @html.tokenize("<a \b>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"\\b\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_u0009_6013" {
  let (tokens, _) = @html.tokenize("<a \t>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_u000a_6014" {
  let (tokens, _) = @html.tokenize("<a \n>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_u000b_6015" {
  let (tokens, _) = @html.tokenize("<a \u{0b}>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"\\u{0b}\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_u000c_6016" {
  let (tokens, _) = @html.tokenize("<a \u{0c}>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_u000d_6017" {
  let (tokens, _) = @html.tokenize("<a \r>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_u001f_6018" {
  let (tokens, _) = @html.tokenize("<a \u{1f}>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"\\u{1f}\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a__6019" {
  let (tokens, _) = @html.tokenize("<a  >")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a__6020" {
  let (tokens, _) = @html.tokenize("<a !>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"!\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a__6021" {
  let (tokens, _) = @html.tokenize("<a \">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"\\\"\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a__6022" {
  let (tokens, _) = @html.tokenize("<a #>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"#\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a__6023" {
  let (tokens, _) = @html.tokenize("<a &>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"&\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a__6024" {
  let (tokens, _) = @html.tokenize("<a '>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"'\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a__6025" {
  let (tokens, _) = @html.tokenize("<a (>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"(\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a__6026" {
  let (tokens, _) = @html.tokenize("<a ->")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"-\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a__6027" {
  let (tokens, _) = @html.tokenize("<a .>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \".\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a__6028" {
  let (tokens, _) = @html.tokenize("<a />")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[], self_closing=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_0_6029" {
  let (tokens, _) = @html.tokenize("<a 0>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"0\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_1_6030" {
  let (tokens, _) = @html.tokenize("<a 1>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"1\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_9_6031" {
  let (tokens, _) = @html.tokenize("<a 9>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"9\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a__6032" {
  let (tokens, _) = @html.tokenize("<a <>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"<\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a__6033" {
  let (tokens, _) = @html.tokenize("<a =>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"=\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a__6034" {
  let (tokens, _) = @html.tokenize("<a >")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a__6035" {
  let (tokens, _) = @html.tokenize("<a ?>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"?\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a__6036" {
  let (tokens, _) = @html.tokenize("<a @>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"@\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6037" {
  let (tokens, _) = @html.tokenize("<a A>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_b_6038" {
  let (tokens, _) = @html.tokenize("<a B>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"b\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_y_6039" {
  let (tokens, _) = @html.tokenize("<a Y>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"y\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_z_6040" {
  let (tokens, _) = @html.tokenize("<a Z>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"z\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a__6041" {
  let (tokens, _) = @html.tokenize("<a [>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"[\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a__6042" {
  let (tokens, _) = @html.tokenize("<a `>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"`\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6043" {
  let (tokens, _) = @html.tokenize("<a a>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au0000_6044" {
  let (tokens, _) = @html.tokenize("<a a\u{00}>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a�\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au0008_6045" {
  let (tokens, _) = @html.tokenize("<a a\b>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\\b\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au0009_6046" {
  let (tokens, _) = @html.tokenize("<a a\t>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au000a_6047" {
  let (tokens, _) = @html.tokenize("<a a\n>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au000b_6048" {
  let (tokens, _) = @html.tokenize("<a a\u{0b}>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\\u{0b}\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au000c_6049" {
  let (tokens, _) = @html.tokenize("<a a\u{0c}>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au000d_6050" {
  let (tokens, _) = @html.tokenize("<a a\r>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au001f_6051" {
  let (tokens, _) = @html.tokenize("<a a\u{1f}>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\\u{1f}\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a__6052" {
  let (tokens, _) = @html.tokenize("<a a >")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_u0000_6053" {
  let (tokens, _) = @html.tokenize("<a a \u{00}>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"�\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_u0008_6054" {
  let (tokens, _) = @html.tokenize("<a a \b>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"\\b\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_u0009_6055" {
  let (tokens, _) = @html.tokenize("<a a \t>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_u000a_6056" {
  let (tokens, _) = @html.tokenize("<a a \n>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_u000b_6057" {
  let (tokens, _) = @html.tokenize("<a a \u{0b}>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"\\u{0b}\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_u000c_6058" {
  let (tokens, _) = @html.tokenize("<a a \u{0c}>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_u000d_6059" {
  let (tokens, _) = @html.tokenize("<a a \r>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_u001f_6060" {
  let (tokens, _) = @html.tokenize("<a a \u{1f}>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"\\u{1f}\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a__6061" {
  let (tokens, _) = @html.tokenize("<a a  >")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a__6062" {
  let (tokens, _) = @html.tokenize("<a a !>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"!\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a__6063" {
  let (tokens, _) = @html.tokenize("<a a \">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"\\\"\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a__6064" {
  let (tokens, _) = @html.tokenize("<a a #>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"#\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a__6065" {
  let (tokens, _) = @html.tokenize("<a a &>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"&\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a__6066" {
  let (tokens, _) = @html.tokenize("<a a '>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"'\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a__6067" {
  let (tokens, _) = @html.tokenize("<a a (>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"(\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a__6068" {
  let (tokens, _) = @html.tokenize("<a a ->")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"-\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a__6069" {
  let (tokens, _) = @html.tokenize("<a a .>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \".\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a__6070" {
  let (tokens, _) = @html.tokenize("<a a />")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_0_6071" {
  let (tokens, _) = @html.tokenize("<a a 0>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"0\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_1_6072" {
  let (tokens, _) = @html.tokenize("<a a 1>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"1\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_9_6073" {
  let (tokens, _) = @html.tokenize("<a a 9>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"9\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a__6074" {
  let (tokens, _) = @html.tokenize("<a a <>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"<\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a__6075" {
  let (tokens, _) = @html.tokenize("<a a =>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a__6076" {
  let (tokens, _) = @html.tokenize("<a a >")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a__6077" {
  let (tokens, _) = @html.tokenize("<a a ?>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"?\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a__6078" {
  let (tokens, _) = @html.tokenize("<a a @>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"@\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_a_6079" {
  let (tokens, _) = @html.tokenize("<a a A>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_b_6080" {
  let (tokens, _) = @html.tokenize("<a a B>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"b\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_y_6081" {
  let (tokens, _) = @html.tokenize("<a a Y>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"y\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_z_6082" {
  let (tokens, _) = @html.tokenize("<a a Z>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"z\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a__6083" {
  let (tokens, _) = @html.tokenize("<a a [>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"[\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a__6084" {
  let (tokens, _) = @html.tokenize("<a a `>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"`\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_a_6085" {
  let (tokens, _) = @html.tokenize("<a a a>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_b_6086" {
  let (tokens, _) = @html.tokenize("<a a b>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"b\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_y_6087" {
  let (tokens, _) = @html.tokenize("<a a y>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"y\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_z_6088" {
  let (tokens, _) = @html.tokenize("<a a z>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"z\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a__6089" {
  let (tokens, _) = @html.tokenize("<a a {>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"{\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_udbc0udc00_6090" {
  let (tokens, _) = @html.tokenize("<a a 􀀀>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}, {name: \"􀀀\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6091" {
  let (tokens, _) = @html.tokenize("<a a!>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a!\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6092" {
  let (tokens, _) = @html.tokenize("<a a\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\\\"\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6093" {
  let (tokens, _) = @html.tokenize("<a a#>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a#\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6094" {
  let (tokens, _) = @html.tokenize("<a a&>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a&\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6095" {
  let (tokens, _) = @html.tokenize("<a a'>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a'\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6096" {
  let (tokens, _) = @html.tokenize("<a a(>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a(\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6097" {
  let (tokens, _) = @html.tokenize("<a a->")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a-\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6098" {
  let (tokens, _) = @html.tokenize("<a a.>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a.\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6099" {
  let (tokens, _) = @html.tokenize("<a a/>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=true), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a0_6100" {
  let (tokens, _) = @html.tokenize("<a a0>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a0\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a1_6101" {
  let (tokens, _) = @html.tokenize("<a a1>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a1\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a9_6102" {
  let (tokens, _) = @html.tokenize("<a a9>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a9\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6103" {
  let (tokens, _) = @html.tokenize("<a a<>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a<\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6104" {
  let (tokens, _) = @html.tokenize("<a a=>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au0000_6105" {
  let (tokens, _) = @html.tokenize("<a a=\u{00}>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"�\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au0008_6106" {
  let (tokens, _) = @html.tokenize("<a a=\b>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\\b\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au0009_6107" {
  let (tokens, _) = @html.tokenize("<a a=\t>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au000a_6108" {
  let (tokens, _) = @html.tokenize("<a a=\n>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au000b_6109" {
  let (tokens, _) = @html.tokenize("<a a=\u{0b}>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\\u{0b}\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au000c_6110" {
  let (tokens, _) = @html.tokenize("<a a=\u{0c}>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au000d_6111" {
  let (tokens, _) = @html.tokenize("<a a=\r>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au001f_6112" {
  let (tokens, _) = @html.tokenize("<a a=\u{1f}>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\\u{1f}\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a__6113" {
  let (tokens, _) = @html.tokenize("<a a= >")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6114" {
  let (tokens, _) = @html.tokenize("<a a=!>")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"!\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6115" {
  let (tokens, _) = @html.tokenize("<a a=\"\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au0000_6116" {
  let (tokens, _) = @html.tokenize("<a a=\"\u{00}\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"�\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au0009_6117" {
  let (tokens, _) = @html.tokenize("<a a=\"\t\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\\t\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au000a_6118" {
  let (tokens, _) = @html.tokenize("<a a=\"\n\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\\n\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au000b_6119" {
  let (tokens, _) = @html.tokenize("<a a=\"\u{0b}\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\\u{0b}\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_au000c_6120" {
  let (tokens, _) = @html.tokenize("<a a=\"\u{0c}\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\\u{0c}\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a__6121" {
  let (tokens, _) = @html.tokenize("<a a=\" \">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \" \"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6122" {
  let (tokens, _) = @html.tokenize("<a a=\"!\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"!\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6123" {
  let (tokens, _) = @html.tokenize("<a a=\"\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6124" {
  let (tokens, _) = @html.tokenize("<a a=\"#\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"#\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6125" {
  let (tokens, _) = @html.tokenize("<a a=\"%\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"%\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6126" {
  let (tokens, _) = @html.tokenize("<a a=\"&\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"&\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6127" {
  let (tokens, _) = @html.tokenize("<a a=\"'\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"'\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6128" {
  let (tokens, _) = @html.tokenize("<a a=\"-\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"-\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6129" {
  let (tokens, _) = @html.tokenize("<a a=\"/\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"/\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a0_6130" {
  let (tokens, _) = @html.tokenize("<a a=\"0\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"0\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a1_6131" {
  let (tokens, _) = @html.tokenize("<a a=\"1\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"1\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a9_6132" {
  let (tokens, _) = @html.tokenize("<a a=\"9\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"9\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6133" {
  let (tokens, _) = @html.tokenize("<a a=\"<\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"<\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6134" {
  let (tokens, _) = @html.tokenize("<a a=\"=\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \"=\"}], self_closing=false), EOF]",
  )
}

///|
test "html5lib/tokenizer/test3_a_a_6135" {
  let (tokens, _) = @html.tokenize("<a a=\">\">")
  inspect(
    tokens,
    content="[StartTag(name=\"a\", attrs=[{name: \"a\", value: \">\"}], self_closing=false), EOF]",
  )
}
